{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 18: Reinforcement Learning\n",
    "\n",
    "**Reference:** Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Aurélien Géron)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Chapter Introduction\n",
    "\n",
    "Reinforcement Learning (RL) is one of the most exciting fields of Machine Learning today. It has been around since the 1950s, but a revolution took place in 2013 when DeepMind demonstrated a system that could learn to play Atari games from scratch, eventually outperforming humans in most of them, using only raw pixels as inputs and without any prior knowledge of the rules. This culminated in the victory of AlphaGo against world champions in the game of Go.\n",
    "\n",
    "In Reinforcement Learning, a software **agent** makes **observations** and takes **actions** within an **environment**, and in return it receives **rewards**. Its objective is to learn to act in a way that will maximize its expected long-term rewards. If you don't mind a bit of anthropomorphism, you can think of positive rewards as pleasure and negative rewards as pain (or punishment). The agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain.\n",
    "\n",
    "In this chapter, we will look at the fundamental concepts of RL, including Markov Decision Processes (MDPs), Q-Learning, and Policy Gradients. We will then dive into Deep Reinforcement Learning, implementing the algorithms that allowed DeepMind to beat Atari games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning to Optimize Rewards\n",
    "\n",
    "In RL, there is no supervisor to tell the agent what is right or wrong. The agent only gets a reward signal (which may be delayed). For example, in a maze, the agent might only get a reward when it escapes. \n",
    "\n",
    "The algorithm used by the agent to determine its actions is called its **policy**. The policy can be a neural network taking observations as inputs and outputting the action to take.\n",
    "\n",
    "### Policy Search\n",
    "How do we find a good policy? \n",
    "* **Genetic Algorithms:** Randomly generate a population of policies, evaluate them, keep the best, and mutate them.\n",
    "* **Policy Gradients:** Evaluate the gradients of the rewards with respect to the policy parameters, then tweak the parameters to follow the gradient toward higher rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. OpenAI Gym\n",
    "\n",
    "OpenAI Gym is a toolkit for developing and comparing Reinforcement Learning algorithms. It provides a wide variety of environments (Atari, board games, physics simulations).\n",
    "\n",
    "Let's create a simple environment: **CartPole**. The goal is to balance a pole on a moving cart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# 1. Create the Environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 2. Reset the environment (returns initial observation)\n",
    "obs = env.reset()\n",
    "print(\"Initial Observation:\", obs)\n",
    "# Observation: [Cart Position, Cart Velocity, Pole Angle, Pole Velocity At Tip]\n",
    "\n",
    "# 3. Render (Note: Rendering often requires a display, which may not work in headless notebooks)\n",
    "# env.render()\n",
    "\n",
    "# 4. Action Space\n",
    "# Discrete(2) means possible actions are 0 (left) and 1 (right)\n",
    "print(\"Action Space:\", env.action_space)\n",
    "\n",
    "# 5. Step\n",
    "action = 1 # Push right\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(\"New Observation:\", obs)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Done:\", done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hardcoded Policy:**\n",
    "A simple strategy: if the pole is tilting left, push the cart left; if right, push right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(50):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)\n",
    "\n",
    "print(\"Mean Reward (Basic Policy):\", np.mean(totals), \"Std:\", np.std(totals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network Policies\n",
    "\n",
    "Ideally, we want a neural network to learn the policy. This network will take the observation as input and output the probability of taking each action.\n",
    "\n",
    "* **Input:** Observation (4 dimensions).\n",
    "* **Output:** Probability of action 0 (Left). (Probability of Right is $1 - p$).\n",
    "* **Selection:** We sample an action based on this probability. Sampling is better than always picking the highest probability action because it allows the agent to **explore** new strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 4\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "print(\"Model built.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating Actions: The Credit Assignment Problem\n",
    "\n",
    "If an agent manages to balance the pole for 100 steps, how do we know *which* of those 100 actions were good and which were bad? This is the **Credit Assignment Problem**.\n",
    "\n",
    "To solve this, we use the **Discount Factor ($\\\\gamma$)**. We evaluate an action based on the sum of all future rewards it led to, but we discount rewards that occur far in the future.\n",
    "\n",
    "$$ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} $$\n",
    "\n",
    "If $\\gamma$ is close to 0, the agent cares only about immediate rewards. If close to 1, it cares about the long term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Policy Gradients (REINFORCE Algorithm)\n",
    "\n",
    "The REINFORCE algorithm works as follows:\n",
    "1.  Let the neural network play the game several times. At each step, calculate gradients that would make the chosen action *more likely*, but don't apply them yet.\n",
    "2.  Compute the return (total discounted reward) for each episode.\n",
    "3.  If an episode's return is better than average (positive advantage), apply the gradients to encourage the actions. If worse (negative advantage), apply opposite gradients to discourage them.\n",
    "4.  Compute the mean of the resulting gradients over all episodes and update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Predict probability of going Left (0)\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        \n",
    "        # Sample action (0 or 1)\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
    "    return obs, reward, done, grads\n",
    "\n",
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads\n",
    "\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std for discounted_rewards in all_discounted_rewards]\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.binary_crossentropy\n",
    "\n",
    "print(\"Policy Gradient functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Markov Decision Processes (MDPs)\n",
    "\n",
    "An MDP consists of a set of states $S$, a set of actions $A$, reward probabilities, and transition probabilities $T(s, a, s')$.\n",
    "\n",
    "**Bellman Optimality Equation:**\n",
    "The optimal value of a state $V^*(s)$ is the sum of the expected immediate reward and the discounted optimal value of the next state.\n",
    "\n",
    "$$ V^*(s) = \\max_a \\sum_{s'} T(s, a, s') [R(s, a, s') + \\gamma V^*(s')] $$\n",
    "\n",
    "**Q-Values:**\n",
    "Instead of valuing states ($V$), we value state-action pairs ($Q$). The optimal Q-Value $Q^*(s, a)$ is the expected return of taking action $a$ in state $s$ and acting optimally thereafter.\n",
    "\n",
    "$$ Q^*(s, a) = \\sum_{s'} T(s, a, s') [R(s, a, s') + \\gamma \\max_{a'} Q^*(s', a')] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Q-Learning\n",
    "\n",
    "If we don't know the transitions $T$, we use **Temporal Difference (TD) Learning**. We explore the environment and update our Q-Value estimates iteratively.\n",
    "\n",
    "**Update Rule:**\n",
    "$$ Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha (r + \\gamma \\max_{a'} Q(s', a')) $$\n",
    "\n",
    "To explore, we use an **$\\epsilon$-greedy policy**: with probability $\\epsilon$, pick a random action; otherwise, pick the best action ($\\text{argmax}_a Q(s, a)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Deep Q-Learning (DQN)\n",
    "\n",
    "In complex environments (like Atari games with raw pixels), the number of states is too huge to store in a Q-table. We use a Deep Neural Network (DQN) to approximate the Q-Value function: $Q(s, a) \\approx Q(s, a; \\theta)$.\n",
    "\n",
    "**Training Loop Stability Tricks:**\n",
    "1.  **Experience Replay:** Instead of training on the latest experience, we store experiences $(s, a, r, s', done)$ in a **Replay Buffer** and sample a random batch for training. This breaks correlations between consecutive steps.\n",
    "2.  **Target Network:** We use two networks. The **Online Model** learns. The **Target Model** defines the target values ($r + \\gamma \\max Q_{target}$). The Target Model weights are copied from the Online Model only periodically (e.g., every 1000 steps). This prevents the target from moving while we are trying to hit it (feedback loops).\n",
    "\n",
    "### Implementation of a DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "batch_size = 32\n",
    "discount_factor = 0.95\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(2)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis])\n",
    "        return np.argmax(Q_values[0])\n",
    "\n",
    "replay_buffer = deque(maxlen=2000)\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, dones\n",
    "\n",
    "def play_one_step_dqn(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    \n",
    "    # Compute target Q values using the Target Model (not implemented here, using model for simplicity)\n",
    "    # In full DQN, next_Q_values = target_model.predict(next_states)\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    target_Q_values = (rewards + (1 - dones) * discount_factor * max_next_Q_values)\n",
    "    \n",
    "    # Compute gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        # Extract Q-value for the specific action taken\n",
    "        one_hot_actions = tf.one_hot(actions, depth=2)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * one_hot_actions, axis=1)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "        \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "print(\"DQN functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. DQN Variants\n",
    "\n",
    "**Double DQN:**\n",
    "Standard DQN tends to overestimate Q-values because the `max` operator uses the same values to select and evaluate an action. Double DQN uses the Online Model to *select* the best action and the Target Model to *evaluate* it.\n",
    "\n",
    "$$ Q_{target} = r + \\gamma Q_{target}(s', \\text{argmax}_{a'} Q_{online}(s', a')) $$\n",
    "\n",
    "**Dueling DQN:**\n",
    "The Q-Value state-action pair can be decomposed into the value of the state $V(s)$ and the advantage of the action $A(s, a)$.\n",
    "$$ Q(s, a) = V(s) + A(s, a) $$\n",
    "In Dueling DQN, the network splits into two streams (one for $V$, one for $A$) and merges them at the end. This allows the network to learn which states are valuable without having to learn the effect of every action for every state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}