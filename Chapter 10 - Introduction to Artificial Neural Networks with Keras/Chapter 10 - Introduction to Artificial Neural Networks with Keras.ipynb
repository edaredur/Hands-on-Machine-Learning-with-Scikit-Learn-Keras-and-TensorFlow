{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
    "\n",
    "**Reference:** Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Aurélien Géron)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Chapter Summary\n",
    "\n",
    "This chapter introduces Artificial Neural Networks (ANNs), the core of Deep Learning. Inspired by biological neurons, ANNs are versatile, powerful, and scalable models ideal for complex tasks like image classification, speech recognition, and natural language processing.\n",
    "\n",
    "**Key Topics:**\n",
    "1.  **From Biological to Artificial Neurons:** The evolution of ANNs from the simple Perceptron to the Multilayer Perceptron (MLP).\n",
    "2.  **Training with Backpropagation:** How modern neural networks learn using Gradient Descent and the reverse-mode autodiff algorithm.\n",
    "3.  **Keras API:** Using the high-level Keras API (integrated into TensorFlow) to build, train, and evaluate deep learning models easily.\n",
    "4.  **Building Models:**\n",
    "    * **Sequential API:** For simple stacks of layers.\n",
    "    * **Functional API:** For complex topologies (multiple inputs/outputs).\n",
    "    * **Subclassing API:** For fully dynamic models.\n",
    "5.  **Hyperparameter Tuning:** Strategies to optimize learning rate, number of layers, neurons per layer, and activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanations\n",
    "\n",
    "### A. The Perceptron\n",
    "\n",
    "**Mechanism:**\n",
    "The Perceptron is one of the simplest ANN architectures (invented in 1957). It is based on a **Threshold Logic Unit (TLU)**.\n",
    "* **Input:** The TLU takes numerical inputs ($x_1, x_2, ..., x_n$), each associated with a weight ($w_1, w_2, ..., w_n$).\n",
    "* **Computation:** It computes a weighted sum of its inputs ($z$).\n",
    "\n",
    "$$ z = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b = \\mathbf{w}^T \\mathbf{x} + b $$\n",
    "\n",
    "* **Activation:** It applies a step function (e.g., Heaviside step function) to this sum and outputs the result ($h_\\mathbf{w}(\\mathbf{x})$).\n",
    "\n",
    "$$ h_\\mathbf{w}(\\mathbf{x}) = \\text{step}(z) $$\n",
    "\n",
    "**Training (Hebb's Rule):**\n",
    "\"Cells that fire together, wire together.\" The Perceptron training rule reinforces the weights of connections that help reduce the error. If a neuron outputs the wrong class, the weights connecting inputs that contributed to this error are adjusted. The weight update rule is:\n",
    "\n",
    "$$ w_{i,j}^{(\\text{next step})} = w_{i,j} + \\eta (y_j - \\hat{y}_j) x_i $$\n",
    "\n",
    "Where:\n",
    "* $w_{i,j}$ is the connection weight between input $i$ and neuron $j$.\n",
    "* $x_i$ is the $i^{th}$ input value.\n",
    "* $\\hat{y}_j$ is the output of neuron $j$.\n",
    "* $y_j$ is the target output of neuron $j$.\n",
    "* $\\eta$ is the learning rate.\n",
    "\n",
    "**Limitation:**\n",
    "Perceptrons can only solve linearly separable problems (like Linear SVMs). They famously cannot solve the XOR problem (Exclusive OR).\n",
    "\n",
    "### B. The Multilayer Perceptron (MLP) and Backpropagation\n",
    "\n",
    "**Mechanism:**\n",
    "An MLP stacks multiple layers of neurons:\n",
    "1.  **Input Layer:** Receives the features.\n",
    "2.  **Hidden Layers:** One or more layers of TLUs where the \"magic\" happens.\n",
    "3.  **Output Layer:** The final layer that produces the prediction.\n",
    "\n",
    "Adding hidden layers allows the network to model complex non-linear relationships, solving problems like XOR.\n",
    "\n",
    "**Backpropagation Algorithm:**\n",
    "How do we train a deep stack of layers? The breakthrough came with Backpropagation (Rumelhart et al., 1986). It is essentially **Gradient Descent** applied efficiently to a multi-layer network using the Chain Rule.\n",
    "\n",
    "1.  **Forward Pass:** The data flows through the network. For each layer $l$, the output is computed as:\n",
    "    $$ \\mathbf{a}^{(l)} = \\phi(\\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}) $$\n",
    "    Where $\\phi$ is the activation function, $\\mathbf{W}$ are the weights, and $\\mathbf{b}$ is the bias.\n",
    "\n",
    "2.  **Compute Loss:** Calculate the error between prediction and target (e.g., using MSE or Cross-Entropy).\n",
    "\n",
    "3.  **Reverse Pass (Error Propagation):** The algorithm propagates the error gradient backward. It calculates the gradient of the loss function with respect to each weight using the chain rule:\n",
    "    $$ \\frac{\\partial J}{\\partial w_{i,j}} = \\frac{\\partial J}{\\partial \\text{output}} \\cdot \\frac{\\partial \\text{output}}{\\partial \\text{hidden}} \\cdot \\dots \\cdot \\frac{\\partial \\text{hidden}}{\\partial w_{i,j}} $$\n",
    "\n",
    "4.  **Update Step:** It tweaks the connection weights to reduce the error using Gradient Descent:\n",
    "    $$ \\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\nabla_{\\mathbf{W}} J(\\mathbf{W}) $$\n",
    "\n",
    "**Crucial Component: Activation Functions:**\n",
    "For backpropagation to work, we need non-linear activation functions (unlike the step function). Common choices:\n",
    "* **Sigmoid (Logistic):** Outputs 0 to 1. Good for probability estimation but suffers from vanishing gradients.\n",
    "    $$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "* **Tanh (Hyperbolic Tangent):** Outputs -1 to 1. Similar to sigmoid but centered around 0, leading to faster convergence.\n",
    "    $$ \\tanh(z) = 2\\sigma(2z) - 1 $$\n",
    "* **ReLU (Rectified Linear Unit):** The default choice for hidden layers. It is fast and reduces vanishing gradient issues.\n",
    "    $$ \\text{ReLU}(z) = \\max(0, z) $$\n",
    "* **Softmax:** Used in the output layer for multiclass classification. It converts scores into probabilities that sum to 1.\n",
    "    $$ \\hat{p}_k = \\frac{\\exp(s_k(\\mathbf{x}))}{\\sum_{j=1}^{K} \\exp(s_j(\\mathbf{x}))} $$\n",
    "\n",
    "### C. Fine-Tuning Neural Network Hyperparameters\n",
    "\n",
    "Neural Networks have many knobs to turn (topology, learning rate, batch size, activation functions). \n",
    "* **Number of Hidden Layers:** Deep networks (many layers) are generally more parameter-efficient than wide networks (one layer with many neurons) for complex tasks because they can learn hierarchical features (e.g., lines -> shapes -> objects).\n",
    "* **Neurons per Layer:** A common strategy is to construct a pyramid (fewer neurons at each subsequent layer), but using the same number of neurons in all hidden layers often works just as well and is simpler to tune.\n",
    "* **Learning Rate ($\\eta$):** The most important hyperparameter. If too low, training is slow. If too high, it diverges.\n",
    "* **Batch Size:** Large batches offer stable gradients and hardware efficiency (GPU) but may generalize worse. Small batches offer noisy gradients (escaping local optima) but are slower per epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Step-by-Step Implementation with Keras\n",
    "\n",
    "### A. Image Classification with the Sequential API\n",
    "We will build a Multilayer Perceptron to classify Fashion MNIST images (grayscale images of clothing items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the Dataset\n",
    "# Fashion MNIST is a drop-in replacement for MNIST (70,000 images, 10 classes, 28x28 pixels).\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# 2. Preprocessing\n",
    "# Split the full training set into a validation set and a (smaller) training set.\n",
    "# Normalize pixel intensities from 0-255 to 0-1 range (essential for Neural Network convergence).\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Class names for reference\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "print(\"Training Data Shape:\", X_train.shape)\n",
    "print(\"Validation Data Shape:\", X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Building the Model using Sequential API\n",
    "# Sequential model is the simplest Keras model for a linear stack of layers.\n",
    "model = keras.models.Sequential([\n",
    "    # Input Layer: Flattens the 28x28 2D array into a 1D array of 784 pixels.\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    \n",
    "    # Hidden Layer 1: Dense layer with 300 neurons.\n",
    "    # Activation='relu' (Rectified Linear Unit) handles non-linearity well.\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    \n",
    "    # Hidden Layer 2: Dense layer with 100 neurons.\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    \n",
    "    # Output Layer: 10 neurons (one per class).\n",
    "    # Activation='softmax' ensures outputs sum to 1 (probabilities for each class).\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# 4. Summary\n",
    "# Displays the network architecture and parameter count.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Compiling the Model\n",
    "# We must specify the loss function, optimizer, and metrics before training.\n",
    "# Loss='sparse_categorical_crossentropy': Used because we have sparse labels (integers 0-9), not one-hot vectors.\n",
    "# Optimizer='sgd': Stochastic Gradient Descent. We can create the object to tune the learning rate.\n",
    "# Metrics=['accuracy']: To track classification accuracy during training.\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# 6. Training the Model\n",
    "# epochs=10: The model sees the entire dataset 30 times.\n",
    "# validation_data: Keras evaluates loss/accuracy on this set at the end of each epoch.\n",
    "# Ideally, you should see 'loss' decrease and 'val_accuracy' increase.\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Visualizing Learning Curves\n",
    "# The fit() method returns a History object containing training parameters and metrics.\n",
    "# We can plot this to detect overfitting (if training loss keeps dropping but validation loss goes up).\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.title(\"Learning Curves\")\n",
    "plt.show()\n",
    "\n",
    "# 8. Evaluation\n",
    "# Evaluate on the unseen Test Set to get the final performance metric.\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Regression MLP\n",
    "We can also use MLPs for regression tasks (e.g., predicting housing prices). The main differences are:\n",
    "* **Output Layer:** Only 1 neuron (for a single predicted value) and **no activation function** (to allow outputting any value range).\n",
    "* **Loss Function:** Mean Squared Error (`mse`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Prepare Data\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Split Train/Valid/Test\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "# Scaling is CRITICAL for Neural Networks to converge efficiently.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 2. Build Regression Model\n",
    "model_reg = keras.models.Sequential([\n",
    "    # Input layer isn't strictly necessary if input_shape is defined in the first Dense layer,\n",
    "    # but it's good practice for clarity. 8 features in California housing data.\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    \n",
    "    # Output layer: 1 neuron, no activation (linear output).\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 3. Compile\n",
    "# Using 'mean_squared_error' for regression loss.\n",
    "model_reg.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "\n",
    "# 4. Train\n",
    "history_reg = model_reg.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "\n",
    "# 5. Predict\n",
    "X_new = X_test[:3]\n",
    "y_pred = model_reg.predict(X_new)\n",
    "print(\"Predictions:\", y_pred)\n",
    "print(\"Actual:\", y_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Functional API for Complex Models\n",
    "The Sequential API is easy but rigid (single input, single output, linear stack). The **Functional API** allows for non-linear topologies, shared layers, and multiple inputs/outputs.\n",
    "\n",
    "Here, we build a **Wide & Deep** neural network. It connects all or part of the inputs directly to the output layer (Wide path) while also sending inputs through a stack of hidden layers (Deep path). This allows the model to learn both simple rules (linear) and complex patterns (deep)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wide & Deep Architecture Implementation\n",
    "\n",
    "# 1. Define Inputs\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "\n",
    "# 2. Deep Path\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "\n",
    "# 3. Concatenate (Wide Path + Deep Path)\n",
    "# We merge the raw input_ directly with the output of hidden2.\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "\n",
    "# 4. Output Layer\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "# 5. Create Model\n",
    "model_wide_deep = keras.models.Model(inputs=[input_], outputs=[output])\n",
    "\n",
    "# Compile and Train (same as before)\n",
    "model_wide_deep.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "model_wide_deep.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Saving and Restoring Models\n",
    "Keras makes it easy to save the entire model (architecture, weights, and optimizer state) to a single HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"my_keras_model.h5\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = keras.models.load_model(\"my_keras_model.h5\")\n",
    "\n",
    "# Verify prediction\n",
    "print(\"Loaded model layers:\", len(loaded_model.layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Callbacks: Early Stopping\n",
    "Instead of guessing the number of epochs, we can use the `EarlyStopping` callback. It interrupts training when the validation loss stops improving, preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Callbacks\n",
    "# 1. ModelCheckpoint: Saves the best model observed during training (lowest validation loss).\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_best_model.h5\", save_best_only=True)\n",
    "\n",
    "# 2. EarlyStopping: Stops if 'val_loss' doesn't improve for 10 epochs (patience).\n",
    "# restore_best_weights=True: Reverts the model weights to the best epoch, not the last one.\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train with callbacks\n",
    "history = model_reg.fit(X_train, y_train, epochs=100,\n",
    "                        validation_data=(X_valid, y_valid),\n",
    "                        callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}