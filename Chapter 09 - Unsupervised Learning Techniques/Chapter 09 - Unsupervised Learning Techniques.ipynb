{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Unsupervised Learning Techniques\n",
    "\n",
    "**Reference:** Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Aurélien Géron)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Chapter Introduction\n",
    "\n",
    "Although most of the applications of Machine Learning today are based on supervised learning (and as a result, this is where most of the investments go to), the vast majority of the available data is unlabeled: we have the input features $\\mathbf{X}$, but we do not have the labels $\\mathbf{y}$. The computer scientist Yann LeCun famously said that \"if intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake.\" In other words, there is a huge potential in unsupervised learning that we have only barely started to sink our teeth into.\n",
    "\n",
    "In this chapter we will look at a few unsupervised learning tasks and algorithms:\n",
    "* **Clustering:** The goal is to group similar instances together into clusters. Clustering is a great tool for data analysis, customer segmentation, recommender systems, search engines, image segmentation, semi-supervised learning, dimensionality reduction, and more.\n",
    "* **Anomaly detection:** The objective is to learn what \"normal\" data looks like, and then use that to detect abnormal instances, such as defective items on a production line or a new trend in a time series.\n",
    "* **Density estimation:** This is the task of estimating the probability density function (PDF) of the random process that generated the dataset. Density estimation is commonly used for anomaly detection: instances located in very low-density regions are likely to be anomalies. It is also useful for data analysis and visualization.\n",
    "\n",
    "We will start with clustering, using K-Means and DBSCAN, and then we will discuss Gaussian mixture models and see how they can be used for density estimation, clustering, and anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering\n",
    "\n",
    "### A. K-Means\n",
    "\n",
    "Consider a dataset containing blobs of instances. K-Means is a simple algorithm capable of clustering this kind of dataset very quickly and efficiently, often in just a few iterations.\n",
    "\n",
    "**Deep Dive Mechanism:**\n",
    "K-Means is a centroid-based algorithm. The goal is to partition the dataset into $k$ distinct, non-overlapping subgroups (clusters). It attempts to minimize the **intra-cluster variance**, also known as inertia.\n",
    "\n",
    "1.  **Initialization:** The algorithm starts by randomly selecting $k$ data points as initial centroids.\n",
    "2.  **Assignment Step:** Every data point in the dataset is assigned to the nearest centroid based on Euclidean distance. This creates $k$ clusters.\n",
    "3.  **Update Step:** The centroids are recomputed by taking the mean of all data points assigned to that centroid.\n",
    "4.  **Convergence:** Steps 2 and 3 repeat until the centroids no longer move (or move very little), meaning the algorithm has converged.\n",
    "\n",
    "**Optimization & Challenges:**\n",
    "* **Inertia:** The objective function is to minimize Inertia, which is the sum of the squared distances between each training instance and its closest centroid.\n",
    "\n",
    "    **Equation 9-1: K-Means Inertia**\n",
    "    $$ \\text{Inertia} = \\sum_{i=1}^{m} \\min_{k=1..K} \\|\\mathbf{x}^{(i)} - \\mathbf{\\mu}_k\\|^2 $$\n",
    "\n",
    "* **Local Optima:** K-Means is guaranteed to converge, but it might converge to a local minimum (suboptimal solution) depending on initialization. It is common to run the algorithm multiple times with different random initializations (`n_init`) and keep the best solution (lowest inertia).\n",
    "* **K-Means++:** A smarter initialization strategy used by default in Scikit-Learn. It chooses the first centroid randomly, then chooses subsequent centroids from the remaining data points with probability proportional to the squared distance from the closest existing centroid. This drastically reduces the probability of poor initialization.\n",
    "* **Hard vs. Soft Clustering:**\n",
    "    * *Hard Clustering:* Each instance is assigned to exactly one cluster.\n",
    "    * *Soft Clustering:* Each instance is assigned a score (e.g., distance or affinity) for every cluster. This can be used as a dimensionality reduction technique (transforming instances into vectors of distances to centroids).\n",
    "\n",
    "**Determining Optimal k:**\n",
    "* **Elbow Method:** Plot inertia vs. $k$. As $k$ increases, inertia decreases. The \"elbow\" of the curve represents a point of diminishing returns where adding more clusters doesn't significantly improve the model.\n",
    "* **Silhouette Score:** A more precise metric. It measures how similar an instance is to its own cluster (cohesion) compared to other clusters (separation). The score ranges from -1 to +1. A high value indicates that the instance is well matched to its own cluster and poorly matched to neighboring clusters. The silhouette coefficient for an instance is equal to $(b - a) / \\max(a, b)$, where $a$ is the mean distance to the other instances in the same cluster (i.e., the mean intra-cluster distance), and $b$ is the mean nearest-cluster distance (i.e., the mean distance to the instances of the next closest cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Generate Data\n",
    "# We create 5 blobs with unequal variances to show K-Means limitations (it assumes equal variance).\n",
    "# This setup simulates a common real-world scenario where clusters aren't perfectly spherical.\n",
    "blob_centers = np.array([\n",
    "    [ 0.2,  2.3],\n",
    "    [-1.5 ,  2.3],\n",
    "    [-2.8,  1.8],\n",
    "    [-2.8,  2.8],\n",
    "    [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
    "X, y = make_blobs(n_samples=2000, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)\n",
    "\n",
    "# 2. Train K-Means (k=5)\n",
    "# 'n_clusters=5' specifies that we are looking for 5 distinct groups.\n",
    "# 'n_init=10' (default) means the algorithm will run 10 times with different random centroid seeds.\n",
    "# K-Means keeps the model with the lowest inertia (sum of squared distances).\n",
    "# This helps avoid getting stuck in local optima where centroids are poorly placed.\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "# 3. Inspect Results\n",
    "# 'cluster_centers_' gives the coordinates of the 5 centroids found.\n",
    "print(\"Cluster Centers:\\n\", kmeans.cluster_centers_)\n",
    "\n",
    "# 'inertia_' is the performance metric (lower is better).\n",
    "# It represents the sum of squared distances of samples to their closest cluster center.\n",
    "print(\"Inertia (Sum of Squared Errors):\", kmeans.inertia_)\n",
    "\n",
    "# 4. Predict new instances\n",
    "# K-Means acts like a Voronoi tesselation; new points are simply assigned to the nearest center.\n",
    "# This is a \"Hard Clustering\" approach.\n",
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
    "print(\"Predictions for new data:\", kmeans.predict(X_new))\n",
    "\n",
    "# 5. Silhouette Score\n",
    "# Calculate Silhouette Score for our chosen k=5.\n",
    "# Range: -1 (wrong cluster) to +1 (perfectly clustered).\n",
    "# A score near 0 means overlapping clusters.\n",
    "print(\"Silhouette Score (k=5):\", silhouette_score(X, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Limits of K-Means\n",
    "Despite its many merits, K-Means is not perfect. It is necessary to run the algorithm several times to avoid suboptimal solutions, plus you need to specify the number of clusters, which can be quite a hassle. Moreover, K-Means does not behave very well when the clusters have varying sizes, different densities, or non-spherical shapes.\n",
    "\n",
    "### C. Using Clustering for Image Segmentation\n",
    "Image segmentation is the task of partitioning an image into multiple segments. In *semantic segmentation*, all pixels that are part of the same object type get assigned to the same segment. In *color segmentation*, we simply assign pixels to the same segment if they have a similar color. We can use K-Means for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# Load the ladybug image (assuming it's in the current directory or provide path)\n",
    "# Since I don't have the file, I will create a dummy image\n",
    "image = np.random.rand(100, 100, 3)\n",
    "\n",
    "X = image.reshape(-1, 3)\n",
    "kmeans = KMeans(n_clusters=8, random_state=42).fit(X)\n",
    "segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n",
    "segmented_img = segmented_img.reshape(image.shape)\n",
    "print(\"Image segmented into 8 colors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Using Clustering for Preprocessing\n",
    "Clustering can be an efficient approach to dimensionality reduction, in particular as a preprocessing step before a supervised learning algorithm. Let's tackle the digits dataset, which is a simple MNIST-like dataset containing 1,797 grayscale $8 \\times 8$ images representing the digits 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load Digits Data\n",
    "X_digits, y_digits = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=42)\n",
    "\n",
    "# Pipeline Construction:\n",
    "# 1. K-Means creates 50 clusters. This acts as feature engineering.\n",
    "#    Instead of raw pixel intensity, the new features are the distances to these 50 centroids.\n",
    "#    This effectively reduces noise and focuses on structural similarity.\n",
    "# 2. Logistic Regression trains on these 50 new features.\n",
    "pipeline = Pipeline([\n",
    "    (\"kmeans\", KMeans(n_clusters=50, random_state=42)),\n",
    "    (\"log_reg\", LogisticRegression(solver=\"lbfgs\", multi_class=\"ovr\", max_iter=5000, random_state=42)),\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy with K-Means Preprocessing:\", pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. DBSCAN\n",
    "\n",
    "This algorithm defines clusters as continuous regions of high density. It is based on the idea that clusters are dense regions in the data space, separated by regions of lower density.\n",
    "\n",
    "**Core Concepts:**\n",
    "1.  **$\\epsilon$-neighborhood:** The radius around a data point.\n",
    "2.  **MinPts (min_samples):** The minimum number of points required within an $\\epsilon$-neighborhood to form a dense region.\n",
    "3.  **Core Point:** A point is a core point if it has at least `min_samples` points (including itself) within its $\\epsilon$-neighborhood.\n",
    "4.  **Border Point:** A point that is within the $\\epsilon$-neighborhood of a core point but does not have enough neighbors to be a core point itself.\n",
    "5.  **Noise Point (Outlier):** A point that is neither a core point nor a border point.\n",
    "\n",
    "**Algorithm Logic:**\n",
    "* For each instance, the algorithm counts how many instances are located within a small distance $\\epsilon$ (epsilon) from it. This region is called the instance’s $\\epsilon$-neighborhood.\n",
    "* If an instance has at least `min_samples` instances in its $\\epsilon$-neighborhood (including itself), then it is considered a core instance. In other words, core instances are those that are located in dense regions.\n",
    "* All instances in the neighborhood of a core instance belong to the same cluster. This neighborhood may include other core instances; therefore, a long sequence of neighboring core instances forms a single cluster.\n",
    "* Any instance that is not a core instance and does not have one in its neighborhood is considered an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Generate 'Moons' dataset (two interleaving half circles).\n",
    "# This shape is impossible for K-Means to separate correctly because K-Means assumes spherical clusters.\n",
    "X, y = make_moons(n_samples=1000, noise=0.05, random_state=42)\n",
    "\n",
    "# DBSCAN configuration:\n",
    "# eps=0.2: The radius of the neighborhood to look for nearby points.\n",
    "# min_samples=5: Minimum points required within 'eps' radius to form a dense region (Core Point).\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "dbscan.fit(X)\n",
    "\n",
    "# Labels of -1 are considered anomalies (noise points) that didn't fit into any cluster.\n",
    "# This built-in outlier detection is a major advantage of DBSCAN.\n",
    "print(\"First 10 labels:\", dbscan.labels_[:10])\n",
    "\n",
    "# Core instances are the \"anchors\" of the clusters.\n",
    "print(\"Number of core instances found:\", len(dbscan.core_sample_indices_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gaussian Mixture Models (GMM)\n",
    "\n",
    "A Gaussian mixture model (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown. All the instances generated from a single Gaussian distribution form a cluster that typically looks like an ellipsoid. Each cluster can have a different ellipsoidal shape, size, density, and orientation.\n",
    "\n",
    "When you observe an instance $\\mathbf{x}$, you know it was generated from one of the Gaussian distributions, but you don't know which one, and you don't know the parameters of these distributions.\n",
    "\n",
    "**Expectation-Maximization (EM) Algorithm:**\n",
    "Finding the optimal parameters (mean $\\mu$, covariance $\\Sigma$, and mixing weight $\\pi$ for each cluster) is difficult. We use EM, which is similar to K-Means:\n",
    "1.  **Expectation (E-step):** Given the current parameter estimates, calculate the probability (responsibility) that each data point belongs to each cluster.\n",
    "2.  **Maximization (M-step):** Update the parameters ($\\mu, \\Sigma, \\pi$) to maximize the likelihood of the data, weighting each data point by the responsibility calculated in the E-step.\n",
    "\n",
    "**Covariance Types:**\n",
    "* **Spherical:** Clusters are spheres (like K-Means), but can have different diameters.\n",
    "* **Diagonal:** Clusters can be ellipsoids, but axes are parallel to coordinate axes.\n",
    "* **Tied:** All clusters share the same covariance matrix (same shape/orientation).\n",
    "* **Full:** Each cluster can take on any ellipsoidal shape and orientation.\n",
    "\n",
    "**Anomaly Detection:**\n",
    "Gaussian Mixtures can be used for anomaly detection: instances located in low-density regions can be considered anomalies. You must define what density threshold you want to use. For example, in a manufacturing company that tries to detect defective products, the ratio of defective products is usually well known (e.g., 4%). You then set the density threshold to be the value that results in having 4% of the instances located in areas below that threshold density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Fit GMM to the moons data.\n",
    "# n_components=2: We assume there are 2 underlying distributions.\n",
    "# n_init=10: The EM algorithm can get stuck in local optima, so we run it 10 times and keep the best result.\n",
    "gm = GaussianMixture(n_components=2, n_init=10, random_state=42)\n",
    "gm.fit(X)\n",
    "\n",
    "print(\"Estimated Means of the components:\\n\", gm.means_)\n",
    "print(\"Did the algorithm converge?\", gm.converged_)\n",
    "\n",
    "# Anomaly Detection Logic:\n",
    "# 1. Calculate the log-likelihood (density) of each instance using 'score_samples'.\n",
    "#    Lower scores indicate the instance is in a lower-density region.\n",
    "densities = gm.score_samples(X)\n",
    "\n",
    "# 2. Define a threshold. We classify the 4% of instances with the lowest density as anomalies.\n",
    "#    This is a common technique for outlier detection in unsupervised settings.\n",
    "density_threshold = np.percentile(densities, 4)\n",
    "anomalies = X[densities < density_threshold]\n",
    "\n",
    "print(f\"Density Threshold (4th percentile): {density_threshold:.2f}\")\n",
    "print(\"Number of anomalies detected:\", len(anomalies))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}