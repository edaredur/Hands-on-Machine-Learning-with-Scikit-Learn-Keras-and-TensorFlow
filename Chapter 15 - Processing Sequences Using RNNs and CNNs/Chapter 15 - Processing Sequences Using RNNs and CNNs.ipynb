{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: Processing Sequences Using RNNs and CNNs\n",
    "\n",
    "**Reference:** Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Aurélien Géron)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Chapter Introduction\n",
    "\n",
    "The batter hits the ball. The outfielder immediately starts running, anticipating the ball’s trajectory. He tracks it, adapts his movements, and finally catches it (under a thunder of applause). Predicting the future is something you do all the time, whether you are finishing a friend’s sentence or anticipating the smell of coffee at breakfast. In this chapter we will discuss recurrent neural networks (RNNs), a class of nets that can predict the future (well, up to a point, of course). They can analyze time series data such as stock prices, and tell you when to buy or sell. In autonomous driving systems, they can anticipate car trajectories and help avoid accidents. More generally, they can work on sequences of arbitrary lengths, rather than on fixed-sized inputs like all the nets we have considered so far. For example, they can take sentences, documents, or audio samples as input, making them extremely useful for natural language processing applications such as automatic translation or speech-to-text.\n",
    "\n",
    "In this chapter we will first look at the fundamental concepts underlying RNNs and how to train them using backpropagation through time, then we will use them to forecast a time series. After that we'll explore the two main difficulties that RNNs face:\n",
    "* **Unstable gradients** (discussed in Chapter 11), which can be alleviated using various techniques, including recurrent dropout and recurrent layer normalization.\n",
    "* **A very limited short-term memory**, which can be extended using LSTMs, GRUs, and 1D convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recurrent Neurons and Layers\n",
    "\n",
    "Up to now, we have focused on feedforward neural networks, where activations flow only in one direction, from the input layer to the output layer. A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let’s look at the simplest possible RNN, composed of just one neuron receiving inputs, producing an output, and sending that output back to itself. At each time step $t$ (also called a frame), this recurrent neuron receives the inputs $\\mathbf{x}_{(t)}$ as well as its own output from the previous time step, $\\hat{y}_{(t-1)}$. Since there is no previous output at the first time step, it is generally set to 0.\n",
    "\n",
    "We can represent this small network against the time axis. This is called *unrolling the network through time*.\n",
    "\n",
    "Each recurrent neuron has two sets of weights: one for the inputs $\\mathbf{x}_{(t)}$ and the other for the outputs of the previous time step $\\hat{y}_{(t-1)}$. Let’s call these weight vectors $\\mathbf{w}_x$ and $\\mathbf{w}_y$. If we consider the whole recurrent layer instead of just one recurrent neuron, we can place all the weight vectors in two weight matrices, $\\mathbf{W}_x$ and $\\mathbf{W}_y$. The output vector of the whole recurrent layer can then be computed as follows:\n",
    "\n",
    "$$ \\hat{\\mathbf{y}}_{(t)} = \\phi(\\mathbf{W}_x^T \\mathbf{x}_{(t)} + \\mathbf{W}_y^T \\hat{\\mathbf{y}}_{(t-1)} + \\mathbf{b}) $$\n",
    "\n",
    "Where:\n",
    "* $\\hat{\\mathbf{y}}_{(t)}$ is the output vector of the recurrent layer at time step $t$.\n",
    "* $\\mathbf{x}_{(t)}$ is the input vector at time step $t$.\n",
    "* $\\mathbf{W}_x$ is the weight matrix for the inputs.\n",
    "* $\\mathbf{W}_y$ is the weight matrix for the outputs of the previous time step.\n",
    "* $\\mathbf{b}$ is the bias vector.\n",
    "* $\\phi$ is the activation function (usually hyperbolic tangent, `tanh`).\n",
    "\n",
    "### Memory Cells\n",
    "\n",
    "Since the output of a recurrent neuron at time step $t$ is a function of all the inputs from previous time steps, you could say it has a form of memory. A part of a neural network that preserves some state across time steps is called a *memory cell* (or simply a cell). The output $\\hat{y}_{(t)}$ is usually the state $\\mathbf{h}_{(t)}$ itself, but complex cells like LSTM separate the two.\n",
    "\n",
    "### Input and Output Sequences\n",
    "\n",
    "An RNN can simultaneously take a sequence of inputs and produce a sequence of outputs (sequence-to-sequence network). This is useful for predicting time series.\n",
    "Alternatively, you could feed the network a sequence of inputs and ignore all outputs except for the last one (sequence-to-vector network). This is useful for sentiment analysis (feed a movie review, output a sentiment score).\n",
    "Conversely, you could feed the network a single input at the first time step (and zeros for all other steps) and let it output a sequence (vector-to-sequence network). This is useful for image captioning.\n",
    "Finally, you could have a Sequence-to-Vector network (Encoder) followed by a Vector-to-Sequence network (Decoder). This is used for translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training RNNs\n",
    "\n",
    "To train an RNN, the trick is to unroll it through time and then simply use regular backpropagation. This strategy is called *Backpropagation Through Time (BPTT)*.\n",
    "\n",
    "Just like in regular backpropagation, there is a first forward pass through the unrolled network. Then the output sequence is evaluated using a cost function $J$ (e.g., MSE for forecasting). The gradients of that cost function are then propagated backward through the unrolled network. Gradients flow backward through time, from the last time step to the first. Note that the weights $\\mathbf{W}_x$, $\\mathbf{W}_y$, and $\\mathbf{b}$ are shared across all time steps, so the gradient for these weights is the sum of the gradients calculated at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forecasting a Time Series\n",
    "\n",
    "Let's create a simple time series function to generate data for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) # wave 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise\n",
    "    return series[..., np.newaxis].astype(np.float32)\n",
    "\n",
    "# Create Training, Validation, and Test sets\n",
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Metrics\n",
    "\n",
    "Before using RNNs, it is good to have a baseline. The simplest baseline is **Naive Forecasting**: predict the last value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = X_valid[:, -1]\n",
    "naive_mse = np.mean(keras.losses.mean_squared_error(y_valid, y_pred))\n",
    "print(\"Naive Forecasting MSE:\", naive_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another simple approach is to use a **Linear Regression** model (or a Dense layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[50, 1]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model_linear.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model_linear.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "print(\"Linear Model evaluated:\", model_linear.evaluate(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Simple RNN\n",
    "\n",
    "A simple RNN layer contains just one recurrent neuron? No! It contains 1 recurrent layer with 1 unit. This unit processes the sequence step by step. Note that by default, `SimpleRNN` uses the `tanh` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
    "])\n",
    "model_rnn.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model_rnn.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "print(\"Simple RNN evaluated:\", model_rnn.evaluate(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep RNNs\n",
    "\n",
    "To stack multiple RNN layers, you must ensure that all intermediate layers return their full sequence of outputs (3D tensor) instead of just the last time step. You do this by setting `return_sequences=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep_rnn = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1)\n",
    "])\n",
    "model_deep_rnn.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model_deep_rnn.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting Several Steps Ahead\n",
    "\n",
    "So far we have predicted only the next time step (1 step ahead). What if we want to predict 10 steps ahead?\n",
    "\n",
    "**Option 1: Autoregressive Prediction**\n",
    "Use the model to predict step 1, add it to the input, predict step 2, and so on. Errors tend to accumulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_time_series(1, n_steps + 10)\n",
    "X_new, y_new = series[:, :n_steps], series[:, n_steps:]\n",
    "X = X_new\n",
    "for step_ahead in range(10):\n",
    "    y_pred_one = model_deep_rnn.predict(X)[:, np.newaxis, :]\n",
    "    X = np.concatenate([X, y_pred_one], axis=1)\n",
    "\n",
    "Y_pred = X[:, n_steps:]\n",
    "print(\"10-step prediction using autoregression complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Seq-to-Vec (Predicting a Vector)**\n",
    "Train the model to output all 10 values at once. We need to regenerate the targets to be vectors of size 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerate data with 10 targets\n",
    "series = generate_time_series(10000, n_steps + 10)\n",
    "X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
    "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
    "X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]\n",
    "\n",
    "model_vector = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(10) # Output 10 values at once\n",
    "])\n",
    "\n",
    "model_vector.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model_vector.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 3: Seq-to-Seq**\n",
    "Instead of predicting 10 steps only at the very end, we can train the model to predict the next 10 steps at *every* single time step. This provides more gradients for training and stabilizes the process. We use `TimeDistributed(Dense(10))` to apply the output layer at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.empty((10000, n_steps, 10)) # each target is a sequence of 10D vectors\n",
    "for step_ahead in range(1, 10 + 1):\n",
    "    Y[..., step_ahead - 1] = series[..., step_ahead:step_ahead + n_steps, 0]\n",
    "Y_train = Y[:7000]\n",
    "Y_valid = Y[7000:9000]\n",
    "Y_test = Y[9000:]\n",
    "\n",
    "model_seq = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "def last_time_step_mse(Y_true, Y_pred):\n",
    "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n",
    "\n",
    "model_seq.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "model_seq.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handling Long Sequences\n",
    "\n",
    "To handle long sequences, we face the vanishing/exploding gradient problem (unstable gradients) and the loss of long-term patterns.\n",
    "\n",
    "### Fighting Unstable Gradients\n",
    "We can use good initialization, faster optimizers, and dropout. For RNNs, Layer Normalization (LN) is more effective than Batch Normalization. LN normalizes across the feature dimension instead of the batch dimension.\n",
    "\n",
    "Here is how to implement Layer Normalization within a custom memory cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNSimpleRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.state_size = units\n",
    "        self.output_size = units\n",
    "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units, activation=None)\n",
    "        self.layer_norm = keras.layers.LayerNormalization()\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    def call(self, inputs, states):\n",
    "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        return norm_outputs, [norm_outputs]\n",
    "\n",
    "model_ln = keras.models.Sequential([\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tackling the Short-Term Memory Problem\n",
    "\n",
    "Due to transformations through time, data is lost. Long-Term memory cells were introduced to solve this.\n",
    "\n",
    "**LSTM (Long Short-Term Memory):**\n",
    "The LSTM cell manages two state vectors: $\\mathbf{h}_{(t)}$ (short-term state) and $\\mathbf{c}_{(t)}$ (long-term state). The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it.\n",
    "\n",
    "It uses three gates controlled by logistic activations:\n",
    "* **Forget Gate ($f_{(t)}$):** Controls which parts of the long-term state should be erased.\n",
    "* **Input Gate ($i_{(t)}$):** Controls which parts of $\\mathbf{g}_{(t)}$ (the main candidate for addition) should be added to the long-term state.\n",
    "* **Output Gate ($o_{(t)}$):** Controls which parts of the long-term state should be read and output to $\\mathbf{h}_{(t)}$ and $\\mathbf{y}_{(t)}$.\n",
    "\n",
    "Equations:\n",
    "$$ \\mathbf{i}_{(t)} = \\sigma(\\mathbf{W}_{xi}^T \\mathbf{x}_{(t)} + \\mathbf{W}_{hi}^T \\mathbf{h}_{(t-1)} + \\mathbf{b}_i) $$\n",
    "$$ \\mathbf{f}_{(t)} = \\sigma(\\mathbf{W}_{xf}^T \\mathbf{x}_{(t)} + \\mathbf{W}_{hf}^T \\mathbf{h}_{(t-1)} + \\mathbf{b}_f) $$\n",
    "$$ \\mathbf{o}_{(t)} = \\sigma(\\mathbf{W}_{xo}^T \\mathbf{x}_{(t)} + \\mathbf{W}_{ho}^T \\mathbf{h}_{(t-1)} + \\mathbf{b}_o) $$\n",
    "$$ \\mathbf{g}_{(t)} = \\tanh(\\mathbf{W}_{xg}^T \\mathbf{x}_{(t)} + \\mathbf{W}_{hg}^T \\mathbf{h}_{(t-1)} + \\mathbf{b}_g) $$\n",
    "$$ \\mathbf{c}_{(t)} = \\mathbf{f}_{(t)} \\otimes \\mathbf{c}_{(t-1)} + \\mathbf{i}_{(t)} \\otimes \\mathbf{g}_{(t)} $$\n",
    "$$ \\mathbf{y}_{(t)} = \\mathbf{h}_{(t)} = \\mathbf{o}_{(t)} \\otimes \\tanh(\\mathbf{c}_{(t)}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.LSTM(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "model_lstm.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "model_lstm.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU (Gated Recurrent Unit):**\n",
    "A simplified version of the LSTM cell. It merges the two state vectors into a single vector $\\mathbf{h}_{(t)}$. It merges the forget and input gates into a single *update gate*. It is computationally more efficient than LSTM and performs similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = keras.models.Sequential([\n",
    "    keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using 1D Convolutional Layers:**\n",
    "1D Conv layers slide a kernel over a sequence. They can learn local patterns in time sequences. By setting `strides` > 1, we can downsample the sequence length, making it easier for an RNN to handle long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv_gru = keras.models.Sequential([\n",
    "    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\",\n",
    "                        input_shape=[None, 1]),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "model_conv_gru.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "model_conv_gru.fit(X_train, Y_train[:, 3::2], epochs=20, validation_data=(X_valid, Y_valid[:, 3::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WaveNet:**\n",
    "Proposed in 2016 by DeepMind researchers. It stacks 1D convolutional layers, doubling the dilation rate (how spread apart the kernel inputs are) at every layer. This allows the network to have an exponentially growing receptive field, capturing extremely long-term patterns efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wavenet = keras.models.Sequential()\n",
    "model_wavenet.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
    "for rate in (1, 2, 4, 8) * 2:\n",
    "    model_wavenet.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
    "                                          activation=\"relu\", dilation_rate=rate))\n",
    "model_wavenet.add(keras.layers.Conv1D(filters=10, kernel_size=1))\n",
    "model_wavenet.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "model_wavenet.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}