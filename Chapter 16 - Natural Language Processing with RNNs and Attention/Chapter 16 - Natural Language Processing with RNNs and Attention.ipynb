{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: Natural Language Processing with RNNs and Attention\n",
    "\n",
    "**Reference:** Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Aurélien Géron)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Chapter Introduction\n",
    "\n",
    "When Alan Turing imagined his famous Turing test in 1950, his objective was to evaluate a machine's ability to match human intelligence. He chose a linguistic task: devising a chatbot capable of fooling its interlocutor into thinking it was human. This highlights that mastering language is arguably Homo sapiens's greatest cognitive ability.\n",
    "\n",
    "A common approach for natural language tasks is to use Recurrent Neural Networks (RNNs). In this chapter, we will explore RNNs for NLP, starting with a **Character RNN** trained to predict the next character in a sentence (allowing us to generate Shakespearean text). Then we will move to **Sentiment Analysis** (classifying movie reviews) using word embeddings. Finally, we will tackle **Neural Machine Translation (NMT)** using Encoder-Decoder architectures, Attention Mechanisms, and ultimately the **Transformer** architecture (Attention Is All You Need)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Shakespearean Text using a Character RNN\n",
    "\n",
    "A Character RNN predicts the next character in a sequence. By feeding the predicted character back into the network, we can generate text.\n",
    "\n",
    "### Creating the Training Dataset\n",
    "\n",
    "First, we download the Shakespeare text, tokenize it (map characters to integers), and create a `tf.data.Dataset` of windowed sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# 1. Download Data\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "\n",
    "# 2. Tokenize (Char Level)\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([shakespeare_text])\n",
    "\n",
    "# Encode the full text as integers\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "max_id = len(tokenizer.word_index) # Number of distinct characters\n",
    "dataset_size = len(encoded)\n",
    "\n",
    "print(f\"Dataset Size: {dataset_size}\")\n",
    "print(f\"Distinct Characters: {max_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting a Sequential Dataset\n",
    "\n",
    "Training an RNN on a sequence of 1 million characters is infeasible (BPTT would need to unroll 1 million steps). We use the `window()` method to split the sequence into smaller windows of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "n_steps = 100 # Window length\n",
    "window_length = n_steps + 1 # Target is input shifted by 1\n",
    "\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "# window() returns a dataset of datasets. flatten() turns it into a dataset of tensors.\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "# Split inputs (X) and targets (Y)\n",
    "dataset = dataset.shuffle(10000).batch(32)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "\n",
    "# One-hot encode the inputs and targets\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id),\n",
    "                              tf.one_hot(Y_batch, depth=max_id)))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Training the Char-RNN Model\n",
    "\n",
    "We use 2 GRU layers. Note that since the output is a probability distribution over the characters, we use a Dense layer with `softmax` activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2), # dropout for inputs\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "# history = model.fit(dataset, epochs=5) # Training takes time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Fake Shakespeare\n",
    "\n",
    "To generate text, we feed a seed character, get the predicted probabilities, sample the next character (using `tf.random.categorical`), and append it to the text. We use a **Temperature** parameter to control randomness:\n",
    "* Low temperature -> The model picks the most likely character (repetitive text).\n",
    "* High temperature -> The model picks less likely characters (more creative but prone to errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)\n",
    "\n",
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :] # Get probas for the last character\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text\n",
    "\n",
    "# print(complete_text(\"t\", temperature=0.2)) # Example usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stateful RNN\n",
    "\n",
    "A standard RNN is *stateless*: the hidden state is reset to zero at the beginning of every batch. A **Stateful RNN** preserves the final state of a batch and uses it as the initial state for the next batch. This allows the model to learn long-term patterns even if the training sequences are short.\n",
    "\n",
    "**Constraint:** Batches must be sequential (Batch $i$ of Epoch $n$ must follow Batch $i-1$ of Epoch $n$). We cannot shuffle the dataset randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis\n",
    "\n",
    "We will classify movie reviews from the IMDB dataset as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\n",
    "\n",
    "# Visualize a review (decoded)\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "print(\" \".join([id_to_word[id_] for id_ in X_train[0][:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Embeddings\n",
    "\n",
    "The reviews have different lengths. We must trim long reviews and pad short ones using `pad_sequences`. Then, we use an **Embedding Layer**. An embedding layer maps each word index to a dense vector (e.g., 128 dimensions). This dense representation captures semantic meaning (e.g., \"king\" and \"queen\" are close in vector space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embed_size = 128\n",
    "\n",
    "model_sentiment = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embed_size, input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_sentiment.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encoder-Decoder Network for Neural Machine Translation\n",
    "\n",
    "For tasks like translation (English to Spanish), the input sequence and output sequence have different lengths. We use an **Encoder-Decoder** architecture.\n",
    "\n",
    "* **Encoder:** An RNN that reads the input sentence and compresses it into a single context vector (the final hidden state).\n",
    "* **Decoder:** An RNN that takes the context vector as its initial state and generates the translation step-by-step.\n",
    "\n",
    "### Bidirectional RNNs\n",
    "A regular RNN only sees the past. A Bidirectional RNN runs two RNNs: one left-to-right, one right-to-left, and concatenates their outputs. This allows the model to understand the context of a word based on what comes *after* it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Architecture (Pseudo-code as we need a formatted translation dataset)\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "encoder_embeddings = keras.layers.Embedding(vocab_size, embed_size)(encoder_inputs)\n",
    "\n",
    "# Bidirectional Encoder\n",
    "encoder = keras.layers.Bidirectional(\n",
    "    keras.layers.LSTM(256, return_state=True))\n",
    "encoder_outputs, state_h_fwd, state_c_fwd, state_h_bwd, state_c_bwd = encoder(encoder_embeddings)\n",
    "\n",
    "# Concatenate states for the Decoder\n",
    "state_h = keras.layers.Concatenate()([state_h_fwd, state_h_bwd])\n",
    "state_c = keras.layers.Concatenate()([state_c_fwd, state_c_bwd])\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_embedding = keras.layers.Embedding(vocab_size, embed_size)(decoder_inputs)\n",
    "decoder_lstm = keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "output = decoder_dense(decoder_outputs)\n",
    "\n",
    "model_nmt = keras.models.Model([encoder_inputs, decoder_inputs], output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attention Mechanisms\n",
    "\n",
    "The Encoder-Decoder model has a bottleneck: the single context vector must hold the meaning of the entire sentence. For long sentences, this fails.\n",
    "\n",
    "**Attention** allows the Decoder to look at *all* the Encoder's hidden states (not just the last one) at every step. It computes a weighted sum of these states, focusing (attending) on the words relevant to the current word being translated.\n",
    "\n",
    "**Bahdanau Attention (Additive):**\n",
    "$$ e_{(t, i)} = \\mathbf{v}^T \\tanh(\\mathbf{W} \\mathbf{s}_{(t-1)} + \\mathbf{V} \\mathbf{h}_{(i)}) $$\n",
    "$$ \\alpha_{(t, i)} = \\text{softmax}(e_{(t, i)}) $$\n",
    "Where $\\alpha$ are the attention weights.\n",
    "\n",
    "### Attention Is All You Need: The Transformer\n",
    "\n",
    "In 2017, the Transformer architecture revolutionized NLP by eliminating RNNs entirely. It relies solely on Attention mechanisms (Self-Attention and Cross-Attention), allowing for massive parallelization.\n",
    "\n",
    "**Positional Encodings:**\n",
    "Since Transformers have no recurrence, they have no sense of order. We add positional encodings (sine/cosine waves) to the embeddings to give the model information about the position of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims % 2 == 1: max_dims += 1\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_embedding[:, :shape[1], :shape[2]]\n",
    "\n",
    "# Multi-Head Attention is available in Keras\n",
    "# attention = keras.layers.MultiHeadAttention(num_heads=8, key_dim=embed_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}