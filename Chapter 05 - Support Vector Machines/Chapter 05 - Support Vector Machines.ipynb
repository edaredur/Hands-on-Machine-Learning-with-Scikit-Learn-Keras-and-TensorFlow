{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Support Vector Machines\n",
    "\n",
    "**Reference:** Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Aurélien Géron)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Chapter Introduction\n",
    "\n",
    "A Support Vector Machine (SVM) is a powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and anyone interested in Machine Learning should have it in their toolbox. SVMs are particularly well suited for classification of complex small- or medium-sized datasets.\n",
    "\n",
    "This chapter will explain the core concepts of SVMs, how to use them, and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear SVM Classification\n",
    "\n",
    "The fundamental idea behind SVMs is best explained with some pictures. Imagine two classes that can clearly be separated easily with a straight line (they are *linearly separable*).\n",
    "\n",
    "You can think of an SVM classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes. This is called **Large Margin Classification**.\n",
    "\n",
    "Notice that adding more training instances \"off the street\" will not affect the decision boundary at all: it is fully determined (or \"supported\") by the instances located on the edge of the street. These instances are called **Support Vectors**.\n",
    "\n",
    "**Feature Scaling is Crucial**\n",
    "SVMs are sensitive to the feature scales. If features have very different scales, the street will be very narrow. Feature scaling (e.g., using Scikit-Learn’s `StandardScaler`) allows the street to be much wider, which generally leads to better performance.\n",
    "\n",
    "### Soft Margin Classification\n",
    "\n",
    "If we strictly impose that all instances must be off the street and on the right side, this is called **Hard Margin Classification**. There are two main issues with hard margin classification:\n",
    "1.  It only works if the data is linearly separable.\n",
    "2.  It is quite sensitive to outliers.\n",
    "\n",
    "To avoid these issues, it is preferable to use a more flexible model. The objective is to find a good balance between keeping the street as wide as possible and limiting the *margin violations* (i.e., instances that end up in the middle of the street or even on the wrong side). This is called **Soft Margin Classification**.\n",
    "\n",
    "In Scikit-Learn’s SVM classes, you can control this balance using the `C` hyperparameter:\n",
    "* **Low C:** A wider street, but more margin violations (high bias, low variance).\n",
    "* **High C:** A narrower street, but fewer margin violations (low bias, high variance).\n",
    "\n",
    "Let's load the Iris dataset, scale the features, and then train a linear SVM model (using the `LinearSVC` class with `C=1` and the *hinge loss* function) to detect Iris virginica flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# 1. Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica\n",
    "\n",
    "# 2. Build the Pipeline\n",
    "# StandardScaling is essential for SVMs to perform well.\n",
    "# LinearSVC is optimized for linear SVMs and is faster than SVC(kernel=\"linear\").\n",
    "# loss=\"hinge\": The standard SVM loss function.\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)),\n",
    "])\n",
    "\n",
    "# 3. Train the model\n",
    "svm_clf.fit(X, y)\n",
    "\n",
    "# 4. Make a prediction\n",
    "print(\"Prediction for [5.5, 1.7]:\", svm_clf.predict([[5.5, 1.7]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Logistic Regression classifiers, SVM classifiers do not output probabilities for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Nonlinear SVM Classification\n",
    "\n",
    "Although linear SVM classifiers are efficient and work surprisingly well in many cases, many datasets are not even close to being linearly separable. One approach to handling nonlinear datasets is to add more features, such as polynomial features; in some cases this can result in a linearly separable dataset.\n",
    "\n",
    "To implement this idea using Scikit-Learn, you can create a `Pipeline` containing a `PolynomialFeatures` transformer, followed by a `StandardScaler` and a `LinearSVC`. Let’s test this on the moons dataset: this is a toy dataset for binary classification in which the data points are shaped as two interleaving half circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# 1. Generate the Moons dataset\n",
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "# 2. Build Pipeline with Polynomial Features\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42))\n",
    "])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)\n",
    "print(\"Training complete on Moons dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel\n",
    "\n",
    "Adding polynomial features is simple to implement and can work great with all sorts of Machine Learning algorithms (not just SVMs), but at a low polynomial degree it cannot deal with very complex datasets, and with a high polynomial degree it creates a huge number of features, making the model too slow.\n",
    "\n",
    "Fortunately, when using SVMs you can apply an almost miraculous mathematical technique called the **kernel trick** (explained later). It makes it possible to get the same result as if you had added many polynomial features, even with very high-degree polynomials, without actually having to add them. So there is no combinatorial explosion of the number of features since you don’t actually add any features. This trick is implemented by the `SVC` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Polynomial Kernel SVM\n",
    "# kernel=\"poly\": Use the polynomial kernel.\n",
    "# degree=3: A 3rd-degree polynomial kernel.\n",
    "# coef0=1: Controls how much the model is influenced by high-degree polynomials versus low-degree polynomials.\n",
    "# C=5: Regularization parameter.\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "])\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian RBF Kernel\n",
    "\n",
    "Another technique to tackle nonlinear problems is to add features computed using a *similarity function* that measures how much each instance resembles a particular *landmark*.\n",
    "\n",
    "The similarity function used is the **Gaussian Radial Basis Function (RBF)**:\n",
    "$$ \\phi_\\gamma(\\mathbf{x}, \\ell) = \\exp(-\\gamma \\|\\mathbf{x} - \\ell\\|^2) $$\n",
    "\n",
    "Where:\n",
    "* $\\ell$ is the landmark.\n",
    "* $\\gamma$ is a hyperparameter that controls the width of the Gaussian curve.\n",
    "\n",
    "Just like the polynomial features method, the similarity features method can be useful with any Machine Learning algorithm, but it may be computationally expensive to compute all the additional features (especially on large training sets). However, once again the kernel trick does its SVM magic: it makes it possible to obtain a similar result as if you had added many similarity features, without actually having to add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian RBF Kernel SVM\n",
    "# kernel=\"rbf\": Use the Gaussian RBF kernel.\n",
    "# gamma=5: Controls the width of the bell curve.\n",
    "#   - High gamma: Narrow bell curve -> Instances range of influence is small -> Irregular decision boundary (Overfitting risk).\n",
    "#   - Low gamma: Wide bell curve -> Instances range of influence is large -> Smoother decision boundary (Underfitting risk).\n",
    "# C=0.001: Regularization parameter.\n",
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "])\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to choose the kernel?**\n",
    "The rule of thumb is to always try the linear kernel first (remember that `LinearSVC` is much faster than `SVC(kernel=\"linear\")`), especially if the training set is very large or if it has plenty of features. If the training set is not too large, you should try the Gaussian RBF kernel as well; it works well in most cases. Then if you have spare time and computing power, you can also experiment with a few other kernels using cross-validation and grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SVM Regression\n",
    "\n",
    "As we mentioned earlier, the SVM algorithm is quite versatile: not only does it support linear and nonlinear classification, but it also supports linear and nonlinear regression. The trick is to reverse the objective: instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible *on* the street while limiting margin violations (i.e., instances *off* the street).\n",
    "\n",
    "The width of the street is controlled by a hyperparameter $\\epsilon$ (epsilon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "# Generate simple linear data\n",
    "np.random.seed(42)\n",
    "m = 50\n",
    "X = 2 * np.random.rand(m, 1)\n",
    "y = (4 + 3 * X + np.random.randn(m, 1)).ravel()\n",
    "\n",
    "# Linear SVM Regression\n",
    "# epsilon=1.5: Defines the width of the street (margin).\n",
    "# Adding more training instances within the margin does not affect the model’s predictions; thus, the model is said to be epsilon-insensitive.\n",
    "svm_reg = LinearSVR(epsilon=1.5, random_state=42)\n",
    "svm_reg.fit(X, y)\n",
    "\n",
    "print(\"Prediction for X=1.0:\", svm_reg.predict([[1.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle nonlinear regression tasks, you can use a kernelized SVM model. Scikit-Learn’s `SVR` class (which supports the kernel trick) is the regression equivalent of the `SVC` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Generate quadratic data\n",
    "m = 100\n",
    "X = 2 * np.random.rand(m, 1) - 1\n",
    "y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)/10).ravel()\n",
    "\n",
    "# Nonlinear SVM Regression (Polynomial Kernel)\n",
    "# kernel=\"poly\": Use a polynomial kernel.\n",
    "# degree=2: 2nd degree polynomial.\n",
    "# C=100: Regularization (High C -> less regularization).\n",
    "# epsilon=0.1: Margin width.\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"scale\")\n",
    "svm_poly_reg.fit(X, y)\n",
    "\n",
    "print(\"Regression complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Under the Hood\n",
    "\n",
    "This section explains how SVMs make predictions and how their training algorithms work, starting with linear SVM classifiers.\n",
    "\n",
    "### Decision Function and Predictions\n",
    "The linear SVM classifier predicts the class of a new instance $\\mathbf{x}$ by simply computing the decision function $\\mathbf{w}^T \\mathbf{x} + b = w_1 x_1 + \\dots + w_n x_n + b$. If the result is positive, the predicted class $\\hat{y}$ is 1, otherwise it is 0.\n",
    "\n",
    "**Equation 5-2: Linear SVM classifier prediction**\n",
    "$$ \\hat{y} = \\begin{cases} 0 & \\text{if } \\mathbf{w}^T \\mathbf{x} + b < 0 \\\\ 1 & \\text{if } \\mathbf{w}^T \\mathbf{x} + b \\ge 0 \\end{cases} $$\n",
    "\n",
    "### Training Objective\n",
    "Consider the slope of the decision function: it is equal to the norm of the weight vector, $\\|\\mathbf{w}\\|_2$. If we divide this slope by 2, the points where the decision function is equal to $\\pm 1$ will be twice as far away from the decision boundary. In other words, dividing the slope by 2 will multiply the margin by 2. The smaller the weight vector $\\mathbf{w}$, the larger the margin.\n",
    "\n",
    "So we want to minimize $\\|\\mathbf{w}\\|_2$ to get a large margin. However, if we also want to avoid any margin violation (hard margin), then we need the decision function to be greater than 1 for all positive training instances, and lower than -1 for all negative training instances. If we define $t^{(i)} = -1$ for negative instances (if $y^{(i)} = 0$) and $t^{(i)} = 1$ for positive instances (if $y^{(i)} = 1$), then we can express this constraint as $t^{(i)}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\ge 1$ for all instances.\n",
    "\n",
    "We can thus express the hard margin linear SVM classifier objective as the constrained optimization problem:\n",
    "\n",
    "**Equation 5-3: Hard margin linear SVM classifier objective**\n",
    "$$ \\begin{aligned} & \\underset{\\mathbf{w}, b}{\\text{minimize}} & & \\frac{1}{2} \\mathbf{w}^T \\mathbf{w} \\\\ & \\text{subject to} & & t^{(i)}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\ge 1 \\quad \\text{for } i = 1, 2, \\dots, m \\end{aligned} $$\n",
    "\n",
    "To get the soft margin objective, we introduce a *slack variable* $\\zeta^{(i)} \\ge 0$ for each instance: $\\zeta^{(i)}$ measures how much the $i^{th}$ instance is allowed to violate the margin. We now have two conflicting objectives: make the slack variables as small as possible to reduce margin violations, and minimize $\\frac{1}{2} \\mathbf{w}^T \\mathbf{w}$ to increase the margin. The hyperparameter $C$ allows us to define the trade-off between these two objectives.\n",
    "\n",
    "**Equation 5-4: Soft margin linear SVM classifier objective**\n",
    "$$ \\begin{aligned} & \\underset{\\mathbf{w}, b, \\mathbf{\\zeta}}{\\text{minimize}} & & \\frac{1}{2} \\mathbf{w}^T \\mathbf{w} + C \\sum_{i=1}^{m} \\zeta^{(i)} \\\\ & \\text{subject to} & & t^{(i)}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\ge 1 - \\zeta^{(i)} \\quad \\text{and} \\quad \\zeta^{(i)} \\ge 0 \\quad \\text{for } i = 1, 2, \\dots, m \\end{aligned} $$\n",
    "\n",
    "### Quadratic Programming\n",
    "The hard margin and soft margin problems are both convex quadratic optimization problems with linear constraints. Such problems are known as *Quadratic Programming* (QP) problems. Many off-the-shelf solvers are available to solve QP problems.\n",
    "\n",
    "### The Dual Problem\n",
    "Given a constrained optimization problem, known as the *primal problem*, it is possible to express a different but closely related problem, called its *dual problem*. The solution to the dual problem typically gives a lower bound to the solution of the primal problem, but under some conditions (which apply here) it can have the same solution.\n",
    "\n",
    "The SVM dual problem is faster to solve than the primal when the number of training instances is smaller than the number of features. More importantly, it makes the **kernel trick** possible.\n",
    "\n",
    "**Equation 5-6: Dual form of the linear SVM objective**\n",
    "$$ \\begin{aligned} & \\underset{\\mathbf{\\alpha}}{\\text{minimize}} & & \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha^{(i)} \\alpha^{(j)} t^{(i)} t^{(j)} \\mathbf{x}^{(i)T} \\mathbf{x}^{(j)} - \\sum_{i=1}^{m} \\alpha^{(i)} \\\\ & \\text{subject to} & & \\alpha^{(i)} \\ge 0 \\quad \\text{for } i = 1, 2, \\dots, m \\end{aligned} $$\n",
    "\n",
    "### Kernelized SVM\n",
    "Suppose you want to apply a 2nd-degree polynomial transformation to a two-dimensional training set. This involves calculating $(x_1^2, \\sqrt{2}x_1 x_2, x_2^2)$. If you apply this mapping $\\phi$, then the dot product of two transformed vectors is:\n",
    "$$ \\phi(\\mathbf{a})^T \\phi(\\mathbf{b}) = (\\mathbf{a}^T \\mathbf{b})^2 $$\n",
    "\n",
    "Notice that the dot product of the transformed vectors is equal to the square of the dot product of the original vectors. This is the key insight: if you apply the transformation $\\phi$ to all training instances, then the dual problem will contain the dot product $\\phi(\\mathbf{x}^{(i)})^T \\phi(\\mathbf{x}^{(j)})$. But if $\\phi$ is the 2nd-degree polynomial transformation defined above, then you can replace this dot product of transformed vectors simply by $(\\mathbf{x}^{(i)T} \\mathbf{x}^{(j)})^2$. \n",
    "\n",
    "So you don’t actually need to transform the training instances at all: just replace the dot product by its square in the Equation 5-6. The result will be strictly the same as if you went through the trouble of transforming the training set, but much more efficient. This trick is called the **Kernel Trick**.\n",
    "\n",
    "The function $K(\\mathbf{a}, \\mathbf{b}) = (\\mathbf{a}^T \\mathbf{b})^2$ is called a 2nd-degree polynomial kernel. In Machine Learning, a *kernel* is a function capable of computing the dot product $\\phi(\\mathbf{a})^T \\phi(\\mathbf{b})$ based only on the original vectors $\\mathbf{a}$ and $\\mathbf{b}$, without having to compute (or even know about) the transformation $\\phi$.\n",
    "\n",
    "**Common Kernels:**\n",
    "* **Linear:** $K(\\mathbf{a}, \\mathbf{b}) = \\mathbf{a}^T \\mathbf{b}$\n",
    "* **Polynomial:** $K(\\mathbf{a}, \\mathbf{b}) = (\\gamma \\mathbf{a}^T \\mathbf{b} + r)^d$\n",
    "* **Gaussian RBF:** $K(\\mathbf{a}, \\mathbf{b}) = \\exp(-\\gamma \\|\\mathbf{a} - \\mathbf{b}\\|^2)$\n",
    "* **Sigmoid:** $K(\\mathbf{a}, \\mathbf{b}) = \\tanh(\\gamma \\mathbf{a}^T \\mathbf{b} + r)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}