{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Training Models\n",
    "\n",
    "**Reference:** Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Aurélien Géron)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Chapter Introduction\n",
    "\n",
    "So far we have treated Machine Learning models and their training algorithms mostly like black boxes. If you went through some of the exercises in the previous chapters, you may have been surprised by how much you can get done without knowing anything about what’s under the hood: you optimized a regression system, you improved a digit image classifier, and you even built a spam classifier from scratch, all this without knowing how they actually work. Indeed, in many situations you don’t really need to know the implementation details.\n",
    "\n",
    "However, having a good understanding of how things work can help you quickly home in on the appropriate model, the right training algorithm to use, and a good set of hyperparameters for your task. Understanding what’s under the hood will also help you debug issues and perform error analysis more efficiently. Lastly, most of the topics discussed in this chapter will be essential in understanding, building, and training neural networks (discussed in Part II of this book).\n",
    "\n",
    "In this chapter we will start by looking at the Linear Regression model, one of the simplest models there is. We will discuss two very different ways to train it:\n",
    "* Using a direct \"closed-form\" equation that directly computes the model parameters that best fit the model to the training set (i.e., the model parameters that minimize the cost function over the training set).\n",
    "* Using an iterative optimization approach, called Gradient Descent (GD), that gradually tweaks the model parameters to minimize the cost function over the training set, eventually converging to the same set of parameters as the first method. We will look at a few variants of Gradient Descent that we will use again and again when we study neural networks: Batch GD, Mini-batch GD, and Stochastic GD.\n",
    "\n",
    "Next we will look at Polynomial Regression, a more complex model that can fit nonlinear datasets. Since this model has more parameters than Linear Regression, it is more prone to overfitting the training data. We will look at how to detect whether or not this is the case, using learning curves, and then we will look at several regularization techniques that can reduce the risk of overfitting the training set.\n",
    "\n",
    "Finally, we will look at two more models that are commonly used for classification tasks: Logistic Regression and Softmax Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression\n",
    "\n",
    "A linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the *bias term* (also called the *intercept term*).\n",
    "\n",
    "**Equation 4-1: Linear Regression model prediction**\n",
    "$$ \\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n $$\n",
    "\n",
    "Where:\n",
    "* $\\hat{y}$ is the predicted value.\n",
    "* $n$ is the number of features.\n",
    "* $x_i$ is the $i^{th}$ feature value.\n",
    "* $\\theta_j$ is the $j^{th}$ model parameter (including the bias term $\\theta_0$ and the feature weights $\\theta_1, \\theta_2, \\dots, \\theta_n$).\n",
    "\n",
    "This can be written much more concisely using a vectorized form:\n",
    "\n",
    "**Equation 4-2: Linear Regression model prediction (vectorized form)**\n",
    "$$ \\hat{y} = h_{\\theta}(\\mathbf{x}) = \\theta^T \\mathbf{x} $$\n",
    "\n",
    "Where:\n",
    "* $\\theta$ is the model’s parameter vector, containing the bias term $\\theta_0$ and the feature weights $\\theta_1$ to $\\theta_n$.\n",
    "* $\\mathbf{x}$ is the instance’s feature vector, containing $x_0$ to $x_n$, with $x_0$ always equal to 1.\n",
    "* $\\theta^T \\mathbf{x}$ is the dot product of the vectors $\\theta$ and $\\mathbf{x}$.\n",
    "* $h_{\\theta}$ is the hypothesis function, using the model parameters $\\theta$.\n",
    "\n",
    "To train a Linear Regression model, we need to find the value of $\\theta$ that minimizes the RMSE. In practice, it is simpler to minimize the Mean Squared Error (MSE) than the RMSE, and it leads to the same result (because the value that minimizes a function also minimizes its square root).\n",
    "\n",
    "**Equation 4-3: MSE cost function for a Linear Regression model**\n",
    "$$ MSE(\\mathbf{X}, h_{\\theta}) = \\frac{1}{m} \\sum_{i=1}^{m} (\\theta^T \\mathbf{x}^{(i)} - y^{(i)})^2 $$\n",
    "\n",
    "### The Normal Equation\n",
    "\n",
    "To find the value of $\\theta$ that minimizes the cost function, there is a closed-form solution — in other words, a mathematical equation that gives the result directly. This is called the *Normal Equation*.\n",
    "\n",
    "**Equation 4-4: Normal Equation**\n",
    "$$ \\hat{\\theta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y} $$\n",
    "\n",
    "Where:\n",
    "* $\\hat{\\theta}$ is the value of $\\theta$ that minimizes the cost function.\n",
    "* $\\mathbf{y}$ is the vector of target values containing $y^{(1)}$ to $y^{(m)}$.\n",
    "\n",
    "Let’s generate some linear-looking data to test this equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate linear-looking data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3x + Gaussian noise\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s compute $\\hat{\\theta}$ using the Normal Equation. We will use the `inv()` function from NumPy’s linear algebra module (`np.linalg`) to compute the inverse of a matrix, and the `dot()` method for matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add x0 = 1 to each instance\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "# Compute Theta using the Normal Equation\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "print(\"Calculated Theta:\\n\", theta_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would have hoped for $\\theta_0 = 4$ and $\\theta_1 = 3$ instead of $\\theta_0 = 4.215$ and $\\theta_1 = 2.770$. Close enough, but the noise made it impossible to recover the exact parameters of the original function.\n",
    "\n",
    "Now we can make predictions using $\\hat{\\theta}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "print(\"Predictions:\\n\", y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Linear Regression using Scikit-Learn is quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "print(\"Intercept:\", lin_reg.intercept_)\n",
    "print(\"Coefficients:\", lin_reg.coef_)\n",
    "print(\"Predictions:\", lin_reg.predict(X_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LinearRegression` class is based on the `scipy.linalg.lstsq()` function (the name stands for \"least squares\"), which you could call directly. This function computes $\\hat{\\theta} = \\mathbf{X}^+ \\mathbf{y}$, where $\\mathbf{X}^+$ is the *pseudoinverse* of $\\mathbf{X}$ (specifically the Moore-Penrose inverse). You can use `np.linalg.pinv()` to compute the pseudoinverse directly. The pseudoinverse itself is computed using a standard matrix factorization technique called *Singular Value Decomposition (SVD)*. This approach is more efficient than computing the Normal Equation and it handles edge cases nicely (e.g., if $\\mathbf{X}^T \\mathbf{X}$ is not invertible).\n",
    "\n",
    "### Computational Complexity\n",
    "The Normal Equation computes the inverse of $\\mathbf{X}^T \\mathbf{X}$, which is an $(n + 1) \\times (n + 1)$ matrix (where $n$ is the number of features). The computational complexity of inverting such a matrix is typically about $O(n^{2.4})$ to $O(n^3)$ (depending on the implementation). Therefore, if you double the number of features, you multiply the computation time by roughly $2^{2.4} = 5.3$ to $2^3 = 8$.\n",
    "\n",
    "On the positive side, this equation is linear with regards to the number of instances in the training set (it is $O(m)$), so it handles large training sets efficiently, provided they can fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent\n",
    "\n",
    "Gradient Descent is a generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. It measures the local gradient of the error function with regards to the parameter vector $\\theta$, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum!\n",
    "\n",
    "An important parameter in Gradient Descent is the size of the steps, determined by the *learning rate* hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time. If the learning rate is too high, you might jump across the valley and end up on the other side, possibly even higher up than you were before. This might make the algorithm diverge, with larger and larger values, failing to find a good solution.\n",
    "\n",
    "### Batch Gradient Descent\n",
    "To implement Gradient Descent, you need to compute the gradient of the cost function with regards to each model parameter $\\theta_j$. In other words, you need to calculate how much the cost function will change if you change $\\theta_j$ just a little bit. This is called a *partial derivative*.\n",
    "\n",
    "**Equation 4-5: Partial derivatives of the cost function**\n",
    "$$ \\frac{\\partial}{\\partial \\theta_j} MSE(\\theta) = \\frac{2}{m} \\sum_{i=1}^{m} (\\theta^T \\mathbf{x}^{(i)} - y^{(i)}) x_j^{(i)} $$\n",
    "\n",
    "Instead of computing these partial derivatives individually, you can use Equation 4-6 to compute them all in one go. The gradient vector, noted $\\nabla_{\\theta} MSE(\\theta)$, contains all the partial derivatives of the cost function (one for each model parameter).\n",
    "\n",
    "**Equation 4-6: Gradient vector of the cost function**\n",
    "$$ \\nabla_{\\theta} MSE(\\theta) = \\frac{2}{m} \\mathbf{X}^T (\\mathbf{X} \\theta - \\mathbf{y}) $$\n",
    "\n",
    "Notice that this formula involves calculations over the full training set $\\mathbf{X}$, at each Gradient Descent step! This is why the algorithm is called *Batch Gradient Descent*: it uses the whole batch of training data at every step (actually, Full Gradient Descent would probably be a better name). As a result it is terribly slow on very large training sets.\n",
    "\n",
    "Once you have the gradient vector, which points uphill, just go in the opposite direction to go downhill. This means subtracting $\\nabla_{\\theta} MSE(\\theta)$ from $\\theta$. This is where the learning rate $\\eta$ comes into play: multiply the gradient vector by $\\eta$ to determine the size of the downhill step.\n",
    "\n",
    "**Equation 4-7: Gradient Descent step**\n",
    "$$ \\theta^{(\\text{next step})} = \\theta - \\eta \\nabla_{\\theta} MSE(\\theta) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1  # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "print(\"Theta found by Batch Gradient Descent:\\n\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is exactly what the Normal Equation found! Gradient Descent worked perfectly. But what if you had used a different learning rate? If the learning rate is too low, the algorithm will eventually reach the solution, but it will take a long time. If the learning rate is too high, the algorithm diverges, jumping all over the place and actually getting further and further away from the solution at every step.\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "Batch Gradient Descent uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large. At the opposite extreme, *Stochastic Gradient Descent* picks a random instance in the training set at every step and computes the gradients based only on that single instance. Obviously, this makes the algorithm much faster since it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration (SGD can be implemented as an out-of-core algorithm).\n",
    "\n",
    "On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much less regular than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down. So once the algorithm stops, the final parameter values are good, but not optimal.\n",
    "\n",
    "When the cost function is very irregular (as in Figure 4-6), this can actually help the algorithm jump out of local minima, so Stochastic Gradient Descent has a better chance of finding the global minimum than Batch Gradient Descent.\n",
    "\n",
    "Therefore, randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum. One solution to this dilemma is to gradually reduce the learning rate. The steps start out large (which helps make quick progress and escape local minima), then get smaller and smaller, allowing the algorithm to settle at the global minimum. This process is called *simulated annealing*, because it resembles the process of annealing in metallurgy where molten metal is slowly cooled down. The function that determines the learning rate at each iteration is called the *learning schedule*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "\n",
    "print(\"Theta found by Stochastic Gradient Descent:\\n\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention we iterate by rounds of $m$ iterations; each round is called an *epoch*. While the Batch Gradient Descent code iterated 1,000 times through the whole training set, this code goes through the training set only 50 times and reaches a very good solution.\n",
    "\n",
    "To perform Linear Regression using SGD with Scikit-Learn, you can use the `SGDRegressor` class, which defaults to optimizing the squared error cost function. The following code runs for maximum 1,000 epochs (`max_iter=1000`) or until the loss drops by less than 1e-3 during one epoch (`tol=1e-3`), starting with a learning rate of 0.1 (`eta0=0.1`), using the default learning schedule (different from the preceding one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "\n",
    "print(\"SGDRegressor Intercept:\", sgd_reg.intercept_)\n",
    "print(\"SGDRegressor Coefficients:\", sgd_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient Descent\n",
    "\n",
    "The last Gradient Descent algorithm we will look at is called *Mini-batch Gradient Descent*. It is quite simple to understand once you know Batch and Stochastic Gradient Descent: at each step, instead of computing the gradients based on the full training set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD computes the gradients on small random sets of instances called *mini-batches*. The main advantage of Mini-batch GD over Stochastic GD is that you can get a performance boost from hardware optimization of matrix operations, especially when using GPUs.\n",
    "\n",
    "The algorithm’s progress in parameter space is less erratic than with SGD, especially with fairly large mini-batches. As a result, Mini-batch GD will end up walking around a bit closer to the minimum than SGD. But, on the other hand, it may be harder for it to escape from local minima (in the case of problems that suffer from local minima, unlike Linear Regression). Figure 4-11 shows the paths taken by the three algorithms in parameter space during training. They all end up near the minimum, but Batch GD’s path stops actually at the minimum, while both Stochastic GD and Mini-batch GD continue to walk around. However, don’t forget that Batch GD takes a lot of time to take each step, and Stochastic GD and Mini-batch GD would also reach the minimum if you used a good learning schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Polynomial Regression\n",
    "\n",
    "What if your data is actually more complex than a simple straight line? Surprisingly, you can actually use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called *Polynomial Regression*.\n",
    "\n",
    "Let’s look at an example. First, let’s generate some nonlinear data, based on a simple quadratic equation (plus some noise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, a straight line will never fit this data properly. So let’s use Scikit-Learn’s `PolynomialFeatures` class to transform our training data, adding the square of each feature in the training set as a new feature (in this case there is just one feature):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "print(\"Original X[0]:\", X[0])\n",
    "print(\"Expanded X_poly[0]:\", X_poly[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_poly` now contains the original feature of `X` plus the square of this feature. Now you can fit a `LinearRegression` model to this extended training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "print(\"Intercept:\", lin_reg.intercept_)\n",
    "print(\"Coefficients:\", lin_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad: the model estimates $\\hat{y} = 0.56x_1^2 + 0.93x_1 + 1.78$, when in fact the original function was $y = 0.5x_1^2 + 1.0x_1 + 2.0 + \\text{Gaussian noise}$.\n",
    "\n",
    "Note that when there are multiple features, Polynomial Regression is capable of finding relationships between features (which is something a plain Linear Regression model cannot do). This is made possible by the fact that `PolynomialFeatures` also adds all combinations of features up to the given degree. For example, if there were two features $a$ and $b$, `PolynomialFeatures` with `degree=3` would not only add the features $a^2$, $a^3$, $b^2$, and $b^3$, but also the combinations $ab$, $a^2b$, and $ab^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learning Curves\n",
    "\n",
    "If you perform high-degree Polynomial Regression, you will likely fit the training data much better than with plain Linear Regression. For example, Figure 4-14 applies a 300-degree polynomial model to the preceding training data, and compares the result with a pure linear model and a quadratic model (2nd-degree polynomial). Notice how the 300-degree polynomial model wiggles around to get as close as possible to the training instances.\n",
    "\n",
    "This high-degree Polynomial Regression model is severely overfitting the training data, while the linear model is underfitting it. The model that will generalize best in this case is the quadratic model. It makes sense since the data was generated using a quadratic model, but in general you won’t know what function generated the data, so how can you decide how complex your model should be? How can you tell that your model is overfitting or underfitting the data?\n",
    "\n",
    "In Chapter 2, you used cross-validation to get an estimate of a model’s generalization performance. If a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then your model is overfitting. If it performs poorly on both, then it is underfitting. This is one way to tell.\n",
    "\n",
    "Another way is to look at the **learning curves**: these are plots of the model’s performance on the training set and the validation set as a function of the training set size (or the training iteration). To generate the plots, simply train the model several times on different sized subsets of the training set. The following code defines a function that plots the learning curves of a model given some training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "    \n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    plt.legend(loc=\"upper right\", fontsize=14)\n",
    "    plt.xlabel(\"Training set size\", fontsize=14)\n",
    "    plt.ylabel(\"RMSE\", fontsize=14)\n",
    "\n",
    "# Plot for Linear Regression (Underfitting)\n",
    "lin_reg = LinearRegression()\n",
    "plot_learning_curves(lin_reg, X, y)\n",
    "plt.axis([0, 80, 0, 3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "* **Training data:** When there are just one or two instances in the training set, the model can fit them perfectly, which is why the curve starts at zero. But as new instances are added to the training set, it becomes impossible for the model to fit the training data perfectly, both because the data is noisy and because it is not linear at all. So the error on the training data goes up until it reaches a plateau, at which point adding new instances to the training set doesn’t make the average error much better or worse.\n",
    "* **Validation data:** When the model is trained on very few training instances, it is incapable of generalizing properly, which is why the validation error is initially quite big. Then as the model is shown more training examples, it learns and thus the validation error slowly goes down. However, once again a straight line cannot do a good job modeling the data, so the error ends up at a plateau, very close to the other curve.\n",
    "\n",
    "These learning curves are typical of an underfitting model. Both curves have reached a plateau; they are close and fairly high.\n",
    "\n",
    "**The Bias/Variance Trade-off**\n",
    "\n",
    "An important theoretical result of statistics and Machine Learning is the fact that a model’s generalization error can be expressed as the sum of three very different errors:\n",
    "1.  **Bias:** This part of the generalization error is due to wrong assumptions, such as assuming that the data is linear when it is actually quadratic. A high-bias model is most likely to underfit the training data.\n",
    "2.  **Variance:** This part is due to the model’s excessive sensitivity to small variations in the training data. A model with many degrees of freedom (such as a high-degree polynomial model) is likely to have high variance, and thus to overfit the training data.\n",
    "3.  **Irreducible error:** This part is due to the noisiness of the data itself. The only way to reduce this part of the error is to clean up the data (e.g., fix the data sources, such as broken sensors, or detect and remove outliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regularized Linear Models\n",
    "\n",
    "As we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. For a linear model, regularization is typically achieved by constraining the weights of the model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net, which implement three different ways to constrain the weights.\n",
    "\n",
    "### Ridge Regression\n",
    "\n",
    "*Ridge Regression* (also called *Tikhonov regularization*) is a regularized version of Linear Regression: a *regularization term* equal to $\\alpha \\sum_{i=1}^{n} \\theta_i^2$ is added to the cost function. This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to evaluate the model’s performance using the unregularized performance measure.\n",
    "\n",
    "**Equation 4-8: Ridge Regression cost function**\n",
    "$$ J(\\theta) = MSE(\\theta) + \\alpha \\frac{1}{2} \\sum_{i=1}^{n} \\theta_i^2 $$\n",
    "\n",
    "Here is how to perform Ridge Regression with Scikit-Learn using a closed-form solution (a variant of Equation 4-4 using a matrix factorization technique by André-Louis Cholesky):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "print(\"Ridge Prediction:\", ridge_reg.predict([[1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And using Stochastic Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg = SGDRegressor(penalty=\"l2\") # \"l2\" indicates that we want to add a regularization term to the cost function equal to half the square of the l2 norm of the weight vector: this is simply Ridge Regression.\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "print(\"SGD Prediction:\", sgd_reg.predict([[1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "\n",
    "*Least Absolute Shrinkage and Selection Operator Regression* (simply called *Lasso Regression*) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the $\\ell_1$ norm of the weight vector instead of half the square of the $\\ell_2$ norm.\n",
    "\n",
    "**Equation 4-10: Lasso Regression cost function**\n",
    "$$ J(\\theta) = MSE(\\theta) + \\alpha \\sum_{i=1}^{n} |\\theta_i| $$\n",
    "\n",
    "An important characteristic of Lasso Regression is that it tends to eliminate the weights of the least important features (i.e., set them to zero). For example, the dashed line in the right plot on Figure 4-18 (with $\\alpha = 10^{-7}$) looks quadratic, almost linear: all the weights for the high-degree polynomial features are equal to zero. In other words, Lasso Regression automatically performs feature selection and outputs a sparse model (i.e., with few nonzero feature weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "print(\"Lasso Prediction:\", lasso_reg.predict([[1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net\n",
    "\n",
    "Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso’s regularization terms, and you can control the mix ratio $r$. When $r = 0$, Elastic Net is equivalent to Ridge Regression, and when $r = 1$, it is equivalent to Lasso Regression.\n",
    "\n",
    "**Equation 4-12: Elastic Net cost function**\n",
    "$$ J(\\theta) = MSE(\\theta) + r \\alpha \\sum_{i=1}^{n} |\\theta_i| + \\frac{1 - r}{2} \\alpha \\sum_{i=1}^{n} \\theta_i^2 $$\n",
    "\n",
    "So when should you use plain Linear Regression (i.e., without any regularization), Ridge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain Linear Regression. Ridge is a good default, but if you suspect that only a few features are actually useful, you should prefer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to zero as discussed. In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "A very different way to regularize iterative learning algorithms such as Gradient Descent is to stop training as soon as the validation error reaches a minimum. This is called *early stopping*. Figure 4-20 shows a complex model (in this case a high-degree Polynomial Regression model) being trained using Batch Gradient Descent. As the epochs go by, the algorithm learns and its prediction error (RMSE) on the training set naturally goes down, and so does its prediction error on the validation set. However, after a while the validation error stops decreasing and actually starts to go back up. This indicates that the model has started to overfit the training data. With early stopping you just stop training as soon as the validation error reaches the minimum. It is such a simple and efficient regularization technique that Geoffrey Hinton called it a \"beautiful free lunch.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Logistic Regression\n",
    "\n",
    "As we discussed in Chapter 1, some regression algorithms can be used for classification (and vice versa). *Logistic Regression* (also called *Logit Regression*) is commonly used to estimate the probability that an instance belongs to a particular class (e.g., what is the probability that this email is spam?). If the estimated probability is greater than 50%, then the model predicts that the instance belongs to that class (called the positive class, labeled \"1\"), or else it predicts that it does not (i.e., it belongs to the negative class, labeled \"0\"). This makes it a binary classifier.\n",
    "\n",
    "### Estimating Probabilities\n",
    "\n",
    "So how does it work? Just like a Linear Regression model, a Logistic Regression model computes a weighted sum of the input features (plus a bias term), but instead of outputting the result directly like the Linear Regression model does, it outputs the *logistic* of this result.\n",
    "\n",
    "**Equation 4-13: Logistic Regression model estimated probability (vectorized form)**\n",
    "$$ \\hat{p} = h_{\\theta}(\\mathbf{x}) = \\sigma(\\mathbf{x}^T \\theta) $$\n",
    "\n",
    "The logistic — noted $\\sigma(\\cdot)$ — is a *sigmoid function* (i.e., S-shaped) that outputs a number between 0 and 1. It is defined as:\n",
    "\n",
    "**Equation 4-14: Logistic function**\n",
    "$$ \\sigma(t) = \\frac{1}{1 + \\exp(-t)} $$\n",
    "\n",
    "Once the Logistic Regression model has estimated the probability $\\hat{p} = h_{\\theta}(\\mathbf{x})$ that an instance $\\mathbf{x}$ belongs to the positive class, it can make its prediction $\\hat{y}$ easily.\n",
    "\n",
    "**Equation 4-15: Logistic Regression model prediction**\n",
    "$$ \\hat{y} = \\begin{cases} 0 & \\text{if } \\hat{p} < 0.5 \\\\ 1 & \\text{if } \\hat{p} \\ge 0.5 \\end{cases} $$\n",
    "\n",
    "### Training and Cost Function\n",
    "\n",
    "Now you know how a Logistic Regression model estimates probabilities and makes predictions. But how is it trained? The objective of training is to set the parameter vector $\\theta$ so that the model estimates high probabilities for positive instances ($y=1$) and low probabilities for negative instances ($y=0$). This idea is captured by the cost function shown in Equation 4-16 for a single training instance $\\mathbf{x}$.\n",
    "\n",
    "**Equation 4-16: Cost function of a single training instance**\n",
    "$$ c(\\theta) = \\begin{cases} -\\log(\\hat{p}) & \\text{if } y = 1 \\\\ -\\log(1 - \\hat{p}) & \\text{if } y = 0 \\end{cases} $$\n",
    "\n",
    "This makes sense because $-\\log(t)$ grows quite large when $t$ approaches 0, so the cost will be large if the model estimates a probability close to 0 for a positive instance, and it will also be very large if the model estimates a probability close to 1 for a negative instance. On the other hand, $-\\log(t)$ is close to 0 when $t$ is close to 1, so the cost will be close to 0 if the estimated probability is close to 0 for a negative instance or close to 1 for a positive instance, which is precisely what we want.\n",
    "\n",
    "The cost function over the whole training set is simply the average cost over all training instances. It can be written in a single expression called the *log loss*.\n",
    "\n",
    "**Equation 4-17: Logistic Regression cost function (log loss)**\n",
    "$$ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(\\hat{p}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{p}^{(i)})] $$\n",
    "\n",
    "The bad news is that there is no known closed-form equation to compute the value of $\\theta$ that minimizes this cost function (there is no equivalent of the Normal Equation). The good news is that this cost function is convex, so Gradient Descent (or any other optimization algorithm) is guaranteed to find the global minimum (if the learning rate is not too large and you wait long enough).\n",
    "\n",
    "### Decision Boundaries\n",
    "\n",
    "Let’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that contains the sepal and petal length and width of 150 iris flowers of three different species: Iris setosa, Iris versicolor, and Iris virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, 3:]  # petal width only\n",
    "y = (iris[\"target\"] == 2).astype(np.int64)  # 1 if Iris virginica, else 0\n",
    "\n",
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the model’s estimated probabilities for flowers with petal widths varying from 0 to 3 cm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris virginica\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Petal width (cm)\", fontsize=14)\n",
    "plt.ylabel(\"Probability\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The petal width of Iris virginica flowers (represented by triangles) ranges from 1.4 cm to 2.5 cm, while the other iris flowers (represented by squares) generally have a smaller petal width, ranging from 0.1 cm to 1.8 cm. Notice that there is a bit of overlap. Above about 2 cm the classifier is highly confident that the flower is an Iris virginica (it outputs a high probability for that class), while below 1 cm it is highly confident that it is not an Iris virginica (high probability for the \"Not Iris virginica\" class). In between these extremes, the classifier is unsure. However, if you ask it to predict the class (using the `predict()` method rather than `predict_proba()`), it will return whichever class is the most likely. Therefore, there is a **decision boundary** at around 1.6 cm where both probabilities are equal to 50%: if the petal width is higher than 1.6 cm, the classifier will predict that the flower is an Iris virginica, or else it will predict that it is not (even if it is not very confident)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Softmax Regression\n",
    "\n",
    "The Logistic Regression model can be generalized to support multiple classes directly, without having to train and combine multiple binary classifiers. This is called *Softmax Regression*, or *Multinomial Logistic Regression*.\n",
    "\n",
    "The idea is quite simple: when given an instance $\\mathbf{x}$, the Softmax Regression model first computes a score $s_k(\\mathbf{x})$ for each class $k$, then estimates the probability of each class by applying the *softmax function* (also called the *normalized exponential*) to the scores.\n",
    "\n",
    "**Equation 4-19: Softmax score for class k**\n",
    "$$ s_k(\\mathbf{x}) = \\mathbf{x}^T \\theta^{(k)} $$\n",
    "\n",
    "Note that each class has its own dedicated parameter vector $\\theta^{(k)}$. All these vectors are typically stored as rows in a parameter matrix $\\Theta$.\n",
    "\n",
    "**Equation 4-20: Softmax function**\n",
    "$$ \\hat{p}_k = \\sigma(\\mathbf{s}(\\mathbf{x}))_k = \\frac{\\exp(s_k(\\mathbf{x}))}{\\sum_{j=1}^{K} \\exp(s_j(\\mathbf{x}))} $$\n",
    "\n",
    "Where:\n",
    "* $K$ is the number of classes.\n",
    "* $\\mathbf{s}(\\mathbf{x})$ is a vector containing the scores of each class for the instance $\\mathbf{x}$.\n",
    "* $\\sigma(\\mathbf{s}(\\mathbf{x}))_k$ is the estimated probability that the instance $\\mathbf{x}$ belongs to class $k$ given the scores of each class for that instance.\n",
    "\n",
    "Just like the Logistic Regression classifier, the Softmax Regression classifier predicts the class with the highest estimated probability (which is simply the class with the highest score).\n",
    "\n",
    "**Equation 4-21: Softmax Regression classifier prediction**\n",
    "$$ \\hat{y} = \\text{argmax}_k \\sigma(\\mathbf{s}(\\mathbf{x}))_k = \\text{argmax}_k s_k(\\mathbf{x}) = \\text{argmax}_k ((\\theta^{(k)})^T \\mathbf{x}) $$\n",
    "\n",
    "The objective is to have a model that estimates a high probability for the target class (and consequently a low probability for the other classes). Minimizing the cost function shown in Equation 4-22, called the *cross entropy*, should lead to this objective because it penalizes the model when it estimates a low probability for a target class. Cross entropy is frequently used to measure how well a set of estimated class probabilities match the target classes.\n",
    "\n",
    "**Equation 4-22: Cross entropy cost function**\n",
    "$$ J(\\Theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} \\log(\\hat{p}_k^{(i)}) $$\n",
    "\n",
    "Where $y_k^{(i)}$ is equal to 1 if the target class for the $i^{th}$ instance is $k$; otherwise, it is equal to 0.\n",
    "\n",
    "Let’s use Softmax Regression to classify the iris flowers into all three classes. Scikit-Learn’s `LogisticRegression` uses one-versus-rest by default when you train it on more than two classes, but you can set the `multi_class` hyperparameter to `\"multinomial\"` to switch it to Softmax Regression. You must also specify a solver that supports Softmax Regression, such as the `\"lbfgs\"` solver (see Scikit-Learn’s documentation for more details). It also applies $\\ell_2$ regularization by default, which you can control using the hyperparameter `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "# Softmax Regression\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, random_state=42)\n",
    "softmax_reg.fit(X, y)\n",
    "\n",
    "print(\"Predicted Class:\", softmax_reg.predict([[5, 2]]))\n",
    "print(\"Predicted Probabilities:\", softmax_reg.predict_proba([[5, 2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the next time you find an iris with 5 cm long and 2 cm wide petals, you can ask your model to tell you what type of iris it is, and it will answer Iris virginica (class 2) with 94.2% probability (or Iris versicolor with 5.8% probability)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}