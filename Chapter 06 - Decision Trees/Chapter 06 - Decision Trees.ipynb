{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Decision Trees\n",
    "\n",
    "**Reference:** Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Aurélien Géron)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Chapter Introduction\n",
    "\n",
    "Like SVMs, Decision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks, and even multioutput tasks. They are powerful algorithms, capable of fitting complex datasets. For example, in Chapter 2 you trained a `DecisionTreeRegressor` model on the California housing dataset, fitting it perfectly (actually, overfitting it).\n",
    "\n",
    "Decision Trees are also the fundamental components of Random Forests (see Chapter 7), which are among the most powerful Machine Learning algorithms available today.\n",
    "\n",
    "In this chapter we will start by discussing how to train, visualize, and make predictions with Decision Trees. Then we will go through the CART training algorithm used by Scikit-Learn, and we will discuss how to regularize trees and use them for regression tasks. Finally, we will discuss some of the limitations of Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training and Visualizing a Decision Tree\n",
    "\n",
    "To understand Decision Trees, let’s build one and take a look at how it makes predictions. The following code trains a `DecisionTreeClassifier` on the iris dataset (see Chapter 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the trained Decision Tree by first using the `export_graphviz()` method to output a graph definition file called `iris_tree.dot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import os\n",
    "\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=\"iris_tree.dot\",\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")\n",
    "print(\"Dot file generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Making Predictions\n",
    "\n",
    "Let’s see how the tree represented in Figure 6-1 makes predictions. Suppose you find an iris flower and you want to classify it. You start at the root node (depth 0, at the top). This node asks whether the flower’s petal length is smaller than 2.45 cm. If it is, then you move down to the root’s left child node (depth 1, left). In this case, it is a leaf node (i.e., it does not have any child nodes), so it does not ask any questions: you can simply look at the predicted class for that node and the Decision Tree predicts that your flower is an Iris setosa (`class=setosa`).\n",
    "\n",
    "Now suppose you find another flower, but this time the petal length is greater than 2.45 cm. You must move down to the root’s right child node (depth 1, right), which is not a leaf node, so it asks another question: is the petal width smaller than 1.75 cm? If it is, then your flower is most likely an Iris versicolor (depth 2, left). If not, it is likely an Iris virginica (depth 2, right). It’s really that simple.\n",
    "\n",
    "**Gini Impurity**\n",
    "\n",
    "A node’s `gini` attribute measures its impurity: a node is \"pure\" (`gini=0`) if all training instances it applies to belong to the same class. For example, since the depth-1 left node applies only to Iris setosa training instances, it is pure and its `gini` score is 0.\n",
    "\n",
    "Equation 6-1 shows how the training algorithm computes the gini score $G_i$ of the $i^{th}$ node.\n",
    "\n",
    "**Equation 6-1: Gini Impurity**\n",
    "$$ G_i = 1 - \\sum_{k=1}^{n} p_{i,k}^2 $$\n",
    "\n",
    "Where:\n",
    "* $p_{i,k}$ is the ratio of class $k$ instances among the training instances in the $i^{th}$ node.\n",
    "\n",
    "**Estimating Class Probabilities**\n",
    "\n",
    "A Decision Tree can also estimate the probability that an instance belongs to a particular class $k$: first it traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of class $k$ in this node. For example, suppose you have found a flower whose petals are 5 cm long and 1.5 cm wide. The corresponding leaf node is the depth-2 left node, so the Decision Tree should output the following probabilities: 0% for Iris setosa (0/54), 90.7% for Iris versicolor (49/54), and 9.3% for Iris virginica (5/54). And of course if you ask it to predict the class, it should output Iris versicolor (class 1) since it has the highest probability. Let’s check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted Probabilities:\", tree_clf.predict_proba([[5, 1.5]]))\n",
    "print(\"Predicted Class:\", tree_clf.predict([[5, 1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The CART Training Algorithm\n",
    "\n",
    "Scikit-Learn uses the *Classification and Regression Tree* (CART) algorithm to train Decision Trees (also called \"growing\" trees). The idea is quite simple: the algorithm first splits the training set into two subsets using a single feature $k$ and a threshold $t_k$ (e.g., \"petal length $\\le$ 2.45 cm\"). How does it choose $k$ and $t_k$? It searches for the pair $(k, t_k)$ that produces the purest subsets (weighted by their size). The cost function that the algorithm tries to minimize is given by Equation 6-2.\n",
    "\n",
    "**Equation 6-2: CART cost function for classification**\n",
    "$$ J(k, t_k) = \\frac{m_{left}}{m} G_{left} + \\frac{m_{right}}{m} G_{right} $$\n",
    "\n",
    "Where:\n",
    "* $G_{left/right}$ measures the impurity of the left/right subset,\n",
    "* $m_{left/right}$ is the number of instances in the left/right subset.\n",
    "\n",
    "Once it has successfully split the training set in two, it splits the subsets using the same logic, then the sub-subsets and so on, recursively. It stops recursing once it reaches the maximum depth (defined by the `max_depth` hyperparameter), or if it cannot find a split that will reduce impurity. A few other hyperparameters control additional stopping conditions (`min_samples_split`, `min_samples_leaf`, `min_weight_fraction_leaf`, and `max_leaf_nodes`).\n",
    "\n",
    "**Computational Complexity**\n",
    "\n",
    "Making predictions requires traversing the Decision Tree from the root to a leaf. Decision Trees are generally approximately balanced, so traversing the Decision Tree requires going through roughly $O(\\log_2(m))$ nodes. Since each node only requires checking the value of one feature, the overall prediction complexity is just $O(\\log_2(m))$, independent of the number of features. So predictions are very fast, even when dealing with large training sets.\n",
    "\n",
    "However, the training algorithm compares all features (or less if `max_features` is set) on all samples at each node. This results in a training complexity of roughly $O(n \\times m \\log(m))$. For small training sets (less than a few thousand instances), Scikit-Learn can speed up training by presorting the data (set `presort=True`), but this slows down training considerably for larger training sets.\n",
    "\n",
    "**Gini Impurity or Entropy?**\n",
    "\n",
    "By default, the `Gini` impurity measure is used, but you can select the `entropy` impurity measure instead by setting the `criterion` hyperparameter to `\"entropy\"`. The concept of entropy originated in thermodynamics as a measure of molecular disorder: entropy approaches zero when molecules are still and well ordered. In Machine Learning, it is frequently used as an impurity measure: a set’s entropy is zero when it contains instances of only one class.\n",
    "\n",
    "**Equation 6-3: Entropy**\n",
    "$$ H_i = -\\sum_{k=1, p_{i,k} \\ne 0}^{n} p_{i,k} \\log_2(p_{i,k}) $$\n",
    "\n",
    "So should you use Gini impurity or entropy? The truth is, most of the time it does not make a big difference: they lead to similar trees. Gini impurity is slightly faster to compute, so it is a good default. However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regularization Hyperparameters\n",
    "\n",
    "Decision Trees make very few assumptions about the training data (as opposed to linear models, which clearly assume that the data is linear, for example). If left unconstrained, the tree structure will adapt itself to the training data, fitting it very closely, and most likely overfitting it. Such a model is often called a *nonparametric model*, not because it does not have any parameters (it often has a lot) but because the number of parameters is not determined prior to training, so the model structure is free to stick closely to the data. In contrast, a *parametric model* such as a linear model has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of underfitting).\n",
    "\n",
    "To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom during training. As you know by now, this is called regularization. The regularization hyperparameters depend on the algorithm used, but generally you can at least restrict the maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the `max_depth` hyperparameter (the default value is `None`, which means unlimited). Reducing `max_depth` will regularize the model and thus reduce the risk of overfitting.\n",
    "\n",
    "The `DecisionTreeClassifier` class has a few other parameters that similarly restrict the shape of the Decision Tree:\n",
    "* `min_samples_split` (the minimum number of samples a node must have before it can be split),\n",
    "* `min_samples_leaf` (the minimum number of samples a leaf node must have),\n",
    "* `min_weight_fraction_leaf` (same as `min_samples_leaf` but expressed as a fraction of the total number of weighted instances),\n",
    "* `max_leaf_nodes` (maximum number of leaf nodes),\n",
    "* `max_features` (maximum number of features that are evaluated for splitting at each node).\n",
    "\n",
    "Increasing `min_*` hyperparameters or reducing `max_*` hyperparameters will regularize the model.\n",
    "\n",
    "Let's test this regularization on the moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Generate a noisy Moons dataset\n",
    "X_moons, y_moons = make_moons(n_samples=100, noise=0.25, random_state=53)\n",
    "\n",
    "# 1. Unconstrained Tree (prone to overfitting)\n",
    "tree_clf1 = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf1.fit(X_moons, y_moons)\n",
    "\n",
    "# 2. Regularized Tree (better generalization)\n",
    "tree_clf2 = DecisionTreeClassifier(min_samples_leaf=4, random_state=42)\n",
    "tree_clf2.fit(X_moons, y_moons)\n",
    "\n",
    "print(\"Training complete for both constrained and unconstrained trees.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regression\n",
    "\n",
    "Decision Trees are also capable of performing regression. Let’s build a regression tree using Scikit-Learn’s `DecisionTreeRegressor` class, training it on a noisy quadratic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Generate quadratic data with noise\n",
    "np.random.seed(42)\n",
    "m = 200\n",
    "X = np.random.rand(m, 1)\n",
    "y = 4 * (X - 0.5) ** 2\n",
    "y = y + np.random.randn(m, 1) / 10\n",
    "\n",
    "# Train Regression Tree\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg.fit(X, y)\n",
    "\n",
    "print(\"Prediction for x=0.6:\", tree_reg.predict([[0.6]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tree looks very similar to the classification tree you built earlier. The main difference is that instead of predicting a class in each node, it predicts a value. For example, suppose you want to make a prediction for a new instance with $x_1 = 0.6$. You traverse the tree starting at the root, and you eventually reach the leaf node that predicts `value=0.1106`. This prediction is simply the average target value of the 110 training instances associated to this leaf node.\n",
    "\n",
    "The CART algorithm works mostly the same way as earlier, except that instead of trying to split the training set in a way that minimizes impurity, it now tries to split the training set in a way that minimizes the MSE.\n",
    "\n",
    "**Equation 6-4: CART cost function for regression**\n",
    "$$ J(k, t_k) = \\frac{m_{left}}{m} MSE_{left} + \\frac{m_{right}}{m} MSE_{right} $$\n",
    "\n",
    "where:\n",
    "$$ MSE_{node} = \\sum_{i \\in node} (\\hat{y}_{node} - y^{(i)})^2 $$\n",
    "$$ \\hat{y}_{node} = \\frac{1}{m_{node}} \\sum_{i \\in node} y^{(i)} $$\n",
    "\n",
    "Just like for classification tasks, Decision Trees are prone to overfitting when dealing with regression tasks. Without any regularization (i.e., using the default hyperparameters), you get the predictions on the left. These predictions are obviously overfitting the training set very badly. Just setting `min_samples_leaf=10` results in a much more reasonable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Instability\n",
    "\n",
    "Hopefully by now you are convinced that Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful. However they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. For example, Figure 6-7 shows a simple linearly separable dataset: on the left, a Decision Tree can split it easily, while on the right, after the dataset is rotated by 45°, the decision boundary looks unnecessarily convoluted. Although both Decision Trees fit the training set perfectly, it is very likely that the model on the right will not generalize well. One way to limit this problem is to use PCA (see Chapter 8), which often results in a better orientation of the training data.\n",
    "\n",
    "More generally, the main issue with Decision Trees is that they are very sensitive to small variations in the training data. For example, if you just remove the widest Iris versicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide) and train a new Decision Tree, you may get the model represented in Figure 6-8. As you can see, it looks very different from the previous Decision Tree (Figure 6-2). Actually, since the training algorithm used by Scikit-Learn is stochastic (it randomly selects the set of features to evaluate at each node), you may get very different models even on the same training data (unless you set the `random_state` hyperparameter).\n",
    "\n",
    "Random Forests can limit this instability by averaging predictions over many trees, as we will see in the next chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}