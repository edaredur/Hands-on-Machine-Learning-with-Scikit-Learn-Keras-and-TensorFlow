{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
    "\n",
    "**Reference:** Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Aurélien Géron)\n",
    "\n",
    "---\n",
    "\n",
    "So far we have used only datasets that fit in memory, but Deep Learning systems are often trained on very large datasets that will not fit in RAM. Ingesting a large dataset and preprocessing it efficiently can be tricky to implement with other Deep Learning libraries, but TensorFlow makes it easy thanks to the Data API: you just create a dataset object, and tell it where to get the data and how to transform it. TensorFlow takes care of all the implementation details, such as multithreading, queuing, batching, and prefetching. Moreover, the Data API works seamlessly with `tf.keras`!\n",
    "\n",
    "In this chapter, we will cover the Data API, the TFRecord format, and how to build custom preprocessing layers and use the standard Keras layers. We will also look at the TensorFlow ecosystem’s related projects: TF Transform (tf.Transform), TF Datasets (TFDS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "First, let's import the necessary modules and configure the environment for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# specific imports for plotting/data\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Data API\n",
    "\n",
    "The whole Data API revolves around the concept of a `Dataset`. This represents a sequence of data items. Usually you will use datasets that gradually read data from disk, but for simplicity let’s create a dataset entirely in RAM using `tf.data.Dataset.from_tensor_slices()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.range(10)  # any data tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can iterate over a dataset’s items like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dataset:\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Transformations\n",
    "\n",
    "Once you have a dataset, you can apply all sorts of transformations to it by calling its transformation methods. Each method returns a new dataset, so you can chain transformations.\n",
    "\n",
    "The following code:\n",
    "1.  **Repeats** the items 3 times.\n",
    "2.  **Batches** them into groups of 7.\n",
    "3.  **Maps** a function (x * 2) to each item.\n",
    "4.  **Unbatches** them back into single items.\n",
    "5.  **Filters** to keep items < 10.\n",
    "6.  **Takes** just the first 3 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "dataset = dataset.map(lambda x: x * 2)\n",
    "dataset = dataset.unbatch().filter(lambda x: x < 10)\n",
    "\n",
    "for item in dataset.take(3):\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the Data\n",
    "\n",
    "Gradient Descent works best when the instances in the training set are independent and identically distributed (IID). The `shuffle()` method uses a buffer to sample elements randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size=3, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interleaving Lines from Multiple Files\n",
    "\n",
    "For a large dataset that does not fit in memory, a simple solution is to split it into multiple files and read them in parallel. \n",
    "\n",
    "First, let's prepare the California Housing dataset and split it into multiple CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_\n",
    "\n",
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]])) \n",
    "                f.write(\"\\n\")\n",
    "    return filepaths\n",
    "\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the files. We can use `tf.data.Dataset.list_files` to create a dataset of file paths, and then `interleave` to read from multiple files at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
    "\n",
    "# Interleave: Read from 5 files at a time, cycle through them\n",
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Pipelining\n",
    "\n",
    "We need a function to parse the CSV lines and apply scaling. We'll wrap this in a reusable function `csv_reader_dataset` that handles the entire pipeline (loading, parsing, shuffling, batching, and prefetching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8 # Number of features\n",
    "\n",
    "def preprocess(line):\n",
    "    # Definitions for CSV columns: 8 floats for X, 1 float for y\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / X_std, y\n",
    "\n",
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training with the Dataset:**\n",
    "We can now simply pass these datasets to Keras's `model.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets\n",
    "train_set = csv_reader_dataset(train_filepaths, batch_size=32)\n",
    "valid_set = csv_reader_dataset(valid_filepaths, batch_size=32)\n",
    "test_set = csv_reader_dataset(test_filepaths, batch_size=32)\n",
    "\n",
    "# Build Model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# Train\n",
    "history = model.fit(train_set, steps_per_epoch=len(X_train) // 32, epochs=5,\n",
    "                    validation_data=valid_set)\n",
    "\n",
    "# Evaluate\n",
    "model.evaluate(test_set, steps=len(X_test) // 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The TFRecord Format\n",
    "\n",
    "The TFRecord format is TensorFlow’s preferred format for storing large amounts of data efficiently. It is a sequence of binary records. We often use it combined with **Protocol Buffers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a TFRecord file\n",
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")\n",
    "\n",
    "# Reading a TFRecord file\n",
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Example Protobufs\n",
    "\n",
    "Typically, we serialize instances as `Example` protocol buffers. This provides a structured format (like a dictionary) inside the binary record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example\n",
    "\n",
    "# 1. Create a structured Example\n",
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
    "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
    "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n",
    "        }))\n",
    "\n",
    "# 2. Serialize and Write\n",
    "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
    "    f.write(person_example.SerializeToString())\n",
    "\n",
    "# 3. Read and Parse\n",
    "feature_description = {\n",
    "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"emails\": tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "\n",
    "def parse(serialized_example):\n",
    "    return tf.io.parse_single_example(serialized_example, feature_description)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"])\n",
    "for serialized_example in dataset:\n",
    "    parsed_example = parse(serialized_example)\n",
    "    print(parsed_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras Preprocessing Layers\n",
    "\n",
    "Handling preprocessing inside the model ensures that the model is portable and avoids training/serving skew. Keras provides specific layers for normalization, text handling, and categorical features.\n",
    "\n",
    "### One-Hot Encoding & Embeddings\n",
    "We can use standard TF operations or Keras layers to handle categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: One-Hot Encoding using a Lookup Table\n",
    "vocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
    "indices = tf.range(len(vocab), dtype=tf.int64)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
    "num_oov_buckets = 2\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)\n",
    "\n",
    "categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n",
    "cat_indices = table.lookup(categories)\n",
    "cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n",
    "print(\"One-Hot Output:\\n\", cat_one_hot.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Embeddings using Keras Layer\n",
    "embedding_dim = 2\n",
    "embedding_layer = keras.layers.Embedding(input_dim=len(vocab) + num_oov_buckets,\n",
    "                                         output_dim=embedding_dim)\n",
    "print(\"Embedding Output:\\n\", embedding_layer(cat_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Preprocessing Layers\n",
    "\n",
    "Common layers include `Normalization`, `Discretization`, `StringLookup`, and `Hashing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Normalization\n",
    "norm_layer = keras.layers.Normalization()\n",
    "norm_layer.adapt(X_train[:5]) # Adapt to a small sample for demo\n",
    "print(\"Normalized sample:\", norm_layer(X_train[:1]).numpy())\n",
    "\n",
    "# 2. Discretization (Binning)\n",
    "age = tf.constant([[10.], [93.], [57.], [18.], [70.], [5.]])\n",
    "discretization = keras.layers.Discretization(bin_boundaries=[18., 50.])\n",
    "print(\"Discretized Bins:\\n\", discretization(age).numpy())\n",
    "\n",
    "# 3. String Lookup\n",
    "cities = tf.constant([\"Auckland\", \"Paris\", \"Paris\", \"San Francisco\"])\n",
    "str_lookup = keras.layers.StringLookup()\n",
    "str_lookup.adapt(cities)\n",
    "print(\"String Indices:\", str_lookup([[\"Paris\"], [\"Auckland\"], [\"Montreal\"]]).numpy())\n",
    "\n",
    "# 4. Hashing (for very large vocabularies)\n",
    "hashing = keras.layers.Hashing(num_bins=10)\n",
    "print(\"Hashed Indices:\", hashing([[\"Paris\"], [\"Tokyo\"], [\"Auckland\"]]).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TensorFlow Datasets (TFDS)\n",
    "\n",
    "Finally, `tensorflow_datasets` is a library that provides a collection of ready-to-use datasets. They are handled as `tf.data` datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets = tfds.load(name=\"mnist\", as_supervised=True)\n",
    "mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]\n",
    "\n",
    "# Transform, shuffle, and batch\n",
    "mnist_train = mnist_train.shuffle(10000).batch(32).prefetch(1)\n",
    "for image, label in mnist_train.take(1):\n",
    "    print(f\"Image shape: {image.shape}, Label shape: {label.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}