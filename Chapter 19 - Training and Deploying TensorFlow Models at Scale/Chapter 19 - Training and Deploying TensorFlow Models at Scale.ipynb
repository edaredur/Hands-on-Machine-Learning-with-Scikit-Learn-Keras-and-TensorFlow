{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
    "\n",
    "**Reference:** Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Aurélien Géron)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Chapter Introduction\n",
    "\n",
    "Once you have a beautiful model that makes amazing predictions, what do you do with it? Well, you need to put it in production! This could be as simple as running the model on a batch of data and perhaps writing a script that runs this model every night. However, it is often much more involved. Various parts of your infrastructure may need to use this model on live data, in which case you probably want to wrap your model in a web service: this way, any part of your infrastructure can query your model at any time using a simple REST API (or some other protocol), as we discussed in Chapter 2. But as time passes, you need to regularly retrain your model on fresh data and push the updated version to production. You must handle model versioning, gracefully transition from one model to the next, possibly roll back to the previous model in case of problems, and perhaps run multiple different models in parallel to perform A/B experiments. If your product becomes successful, your service may start to get plenty of queries per second (QPS), and it must scale up to support the load. A great solution to scale up your service, as we will see in this chapter, is to use TF Serving, either on your own hardware infrastructure or via a cloud service such as Google Cloud AI Platform.\n",
    "\n",
    "But deployment is not the only challenge. Training large models on huge datasets can take days or even weeks. To speed this up, you can use distributed training strategies to spread the workload across multiple GPUs or TPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Serving a TensorFlow Model\n",
    "\n",
    "### Saving and Exporting\n",
    "\n",
    "Before we deploy, we need a trained model. Let's train a simple MNIST classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load MNIST\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train_full = X_train_full[..., np.newaxis].astype(\"float32\") / 255.\n",
    "X_test = X_test[..., np.newaxis].astype(\"float32\") / 255.\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Build Model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=3, validation_data=(X_valid, y_valid))\n",
    "\n",
    "# Save Model in SavedModel format\n",
    "model_version = \"0001\"\n",
    "model_name = \"my_mnist_model\"\n",
    "model_path = os.path.join(model_name, model_version)\n",
    "tf.saved_model.save(model, model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SavedModel format contains:\n",
    "* `saved_model.pb`: The computation graph definition.\n",
    "* `variables/`: The model weights.\n",
    "* `assets/`: Extra files (e.g., vocabulary files).\n",
    "\n",
    "### TF Serving\n",
    "\n",
    "TensorFlow Serving is a high-performance serving system for machine learning models, designed for production environments. It handles model versioning (loading new versions automatically) and efficiently handles requests via REST or gRPC.\n",
    "\n",
    "To use it, you typically run it inside a Docker container. (The following is a command line instruction, not python code).\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -p 8500:8500 -p 8501:8501 \\\n",
    "    -v \"/path/to/my_mnist_model:/models/my_mnist_model\" \\\n",
    "    -e MODEL_NAME=my_mnist_model \\\n",
    "    tensorflow/serving\n",
    "```\n",
    "\n",
    "### Querying via REST API\n",
    "\n",
    "Once the server is running (listening on port 8501 for REST), we can send HTTP POST requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# Prepare data payload (must be JSON serializable)\n",
    "input_data_json = json.dumps({\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": X_test[:3].tolist()  # Convert numpy array to list\n",
    "})\n",
    "\n",
    "# In a real scenario with Docker running, you would do this:\n",
    "# SERVER_URL = \"http://localhost:8501/v1/models/my_mnist_model:predict\"\n",
    "# response = requests.post(SERVER_URL, data=input_data_json)\n",
    "# response.raise_for_status()\n",
    "# response = response.json()\n",
    "# y_proba = np.array(response[\"predictions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploying to Mobile or Embedded Devices (TFLite)\n",
    "\n",
    "For edge devices (phones, IoT), standard TensorFlow models are too heavy. TFLite optimizes the model for size and latency.\n",
    "\n",
    "### Converting to TFLite\n",
    "\n",
    "We use the `TFLiteConverter`. We can apply **Quantization** (reducing float32 to float16 or int8) to dramatically shrink the model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the SavedModel\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(model_path)\n",
    "\n",
    "# Optimization: Float16 Quantization (reduces size by 2x, minimal accuracy loss)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the .tflite file\n",
    "with open(\"my_mnist_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"TFLite model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Interpreter\n",
    "\n",
    "We can test the `.tflite` model in Python using the Interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"my_mnist_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Run inference on one image\n",
    "input_data = X_test[0:1] # shape (1, 28, 28, 1)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "print(\"TFLite Prediction Class:\", np.argmax(output_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running on the Browser (TensorFlow.js)\n",
    "\n",
    "You can convert models to run directly in a web browser using `tensorflowjs`. This allows for privacy (data stays on the user's device) and low latency.\n",
    "\n",
    "Command line utility:\n",
    "```bash\n",
    "tensorflowjs_converter --input_format=tf_saved_model \\\n",
    "    ./my_mnist_model/0001 ./my_tfjs_model\n",
    "```\n",
    "This produces a `model.json` and binary shard files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distributed Training\n",
    "\n",
    "When a model is too large or data is too vast, we scale horizontally.\n",
    "\n",
    "### Data Parallelism vs. Model Parallelism\n",
    "* **Data Parallelism:** Replicate the model on every GPU. Split the batch into mini-batches, one for each GPU. Gradients are computed independently and then aggregated (summed) to update all models simultaneously.\n",
    "* **Model Parallelism:** Split the model itself across GPUs (e.g., layers 1-5 on GPU0, 6-10 on GPU1). Harder to implement efficiently due to communication overhead.\n",
    "\n",
    "### Mirrored Strategy\n",
    "\n",
    "The standard approach for single-machine, multi-GPU training. It uses the **AllReduce** algorithm to synchronize gradients efficiently across GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use distributed training, wrap model creation and compilation in the strategy scope.\n",
    "# If you have multiple GPUs, TF will utilize them automatically.\n",
    "# If you only have a CPU, it will fallback gracefully.\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "with strategy.scope():\n",
    "    model_dist = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
    "        keras.layers.Dense(100, activation=\"relu\"),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    model_dist.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "                       metrics=[\"accuracy\"])\n",
    "\n",
    "# Training looks exactly the same\n",
    "batch_size = 100 # Should be larger to exploit parallelism\n",
    "model_dist.fit(X_train, y_train, epochs=2, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Strategies\n",
    "\n",
    "* **MultiWorkerMirroredStrategy:** Similar to MirroredStrategy but across multiple servers (nodes).\n",
    "* **TPUStrategy:** Specifically for Google Cloud TPUs. TPUs are specialized hardware accelerators for matrix multiplication, offering immense speedups for Deep Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}