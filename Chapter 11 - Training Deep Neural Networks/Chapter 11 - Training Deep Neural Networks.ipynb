{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Training Deep Neural Networks\n",
    "\n",
    "**Reference:** Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Aurélien Géron)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Chapter Summary\n",
    "\n",
    "Training a shallow neural network is straightforward, but training a Deep Neural Network (DNN) with tens of layers and millions of parameters comes with significant challenges. This chapter addresses the \"Four Horsemen\" of deep learning training difficulties and their solutions.\n",
    "\n",
    "**Key Challenges & Solutions:**\n",
    "1.  **Vanishing/Exploding Gradients:** Gradients shrink or grow uncontrollably as they propagate back through deep layers, making lower layers hard to train.\n",
    "    * *Solutions:* Better initialization strategies (Glorot, He), non-saturating activation functions (ReLU, ELU, SELU), and Batch Normalization.\n",
    "2.  **Lack of Data:** Deep networks need massive data.\n",
    "    * *Solutions:* Transfer Learning (reusing parts of pretrained networks) and Unsupervised Pretraining.\n",
    "3.  **Slow Training:** Gradient Descent is slow.\n",
    "    * *Solutions:* Fast optimizers (Momentum, RMSProp, Adam) and Learning Rate Scheduling.\n",
    "4.  **Overfitting:** Models with millions of parameters easily memorize training data.\n",
    "    * *Solutions:* Regularization techniques ($l_1$/$l_2$ regularization, Dropout, Max-Norm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Explanations\n",
    "\n",
    "### A. Vanishing and Exploding Gradients\n",
    "\n",
    "**The Problem:**\n",
    "During backpropagation, error gradients are propagated from the output layer to the input layer. By the Chain Rule, deep layers involve multiplying many small numbers (derivatives). If these derivatives are $<1$, the gradient shrinks to zero (**Vanishing**). If $>1$, it grows to infinity (**Exploding**).\n",
    "\n",
    "**Solution 1: Weight Initialization**\n",
    "Glorot and Bengio (2010) proposed that for signals to flow properly, the variance of the outputs of a layer should equal the variance of its inputs. This leads to specific initialization strategies depending on the activation function:\n",
    "\n",
    "* **Glorot (Xavier) Initialization:** For Sigmoid/Tanh/Softmax. Sample weights from a normal distribution with variance $\\sigma^2 = \\frac{1}{fan_{avg}}$.\n",
    "* **He Initialization:** For ReLU and its variants. Variance $\\sigma^2 = \\frac{2}{fan_{in}}$.\n",
    "\n",
    "**Solution 2: Non-Saturating Activation Functions**\n",
    "The Sigmoid function saturates at 0 and 1, killing gradients. Alternatives include:\n",
    "\n",
    "* **ReLU (Rectified Linear Unit):** $ReLU(z) = \\max(0, z)$. Fast and non-saturating for positive values. *Issue:* Dying ReLUs (output 0 for all inputs).\n",
    "* **Leaky ReLU:** $LReLU_\\alpha(z) = \\max(\\alpha z, z)$. Ensures neurons never die.\n",
    "* **ELU (Exponential Linear Unit):** Smoother than ReLU, converges faster, but computationally expensive.\n",
    "    $$ \\text{ELU}_\\alpha(z) = \\begin{cases} \\alpha(\\exp(z) - 1) & \\text{if } z < 0 \\\\ z & \\text{if } z \\ge 0 \\end{cases} $$\n",
    "* **SELU (Scaled ELU):** Self-normalizing network if used with LeCun Normal initialization.\n",
    "\n",
    "**Solution 3: Batch Normalization (BN)**\n",
    "Addresses Internal Covariate Shift (distribution of inputs to a layer changes during training). It zero-centers and normalizes each input, then scales and shifts the result using two learnable parameters ($\\\\gamma, \\\\beta$) per layer.\n",
    "\n",
    "$$ \\hat{x}^{(i)} = \\frac{x^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} $$\n",
    "$$ z^{(i)} = \\gamma \\otimes \\hat{x}^{(i)} + \\beta $$\n",
    "\n",
    "### B. Faster Optimizers\n",
    "\n",
    "Standard SGD is slow. We can accelerate it using momentum.\n",
    "\n",
    "* **Momentum Optimization:** Accumulates a momentum vector $\\mathbf{m}$ (like a ball rolling down a hill). $\\beta$ is the momentum (friction).\n",
    "    $$ \\mathbf{m} \\leftarrow \\beta \\mathbf{m} - \\eta \\nabla_\\theta J(\\theta) $$\n",
    "    $$ \\theta \\leftarrow \\theta + \\mathbf{m} $$\n",
    "* **RMSProp:** Adapts the learning rate by dividing by the square root of the exponential moving average of squared gradients. It slows down along steep dimensions.\n",
    "* **Adam (Adaptive Moment Estimation):** Combines Momentum and RMSProp. It tracks both the first moment (mean) and second moment (variance) of the gradients.\n",
    "\n",
    "### C. Learning Rate Scheduling\n",
    "\n",
    "Instead of a constant learning rate $\\eta$, we start high and reduce it over time.\n",
    "* **Power Scheduling:** $\\eta(t) = \\eta_0 / (1 + t/s)^c$\n",
    "* **Exponential Scheduling:** $\\eta(t) = \\eta_0 0.1^{t/s}$\n",
    "* **Performance Scheduling:** Reduce $\\eta$ when validation error stops improving (e.g., `ReduceLROnPlateau`).\n",
    "\n",
    "### D. Avoiding Overfitting: Regularization\n",
    "\n",
    "* **$l_1$ and $l_2$ Regularization:** Adding a penalty term to the loss function (sum of absolute weights for $l_1$, sum of squared weights for $l_2$).\n",
    "* **Dropout:** At every training step, every neuron has a probability $p$ (dropout rate) of being temporarily \"dropped out\" (ignored). This forces the network to be robust and acts like an ensemble of exponentially many networks.\n",
    "* **Max-Norm:** Constrains the weights $w$ such that $\\|w\\|_2 \\le r$. Updates weights by clipping them after each training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Step-by-Step Implementation with Keras\n",
    "\n",
    "### A. Initialization and Activation Functions\n",
    "We will create a model using **He Normal** initialization and the **Leaky ReLU** activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load Fashion MNIST for demonstration\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_valid = X_train_full[:-5000] / 255.0, X_train_full[-5000:] / 255.0\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "# Building a model with He Initialization and Leaky ReLU\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    \n",
    "    # He Normal initialization is optimal for ReLU-based activations.\n",
    "    # We separate the activation layer to use the advanced LeakyReLU layer.\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    \n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    \n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile and train (standard SGD for now)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Model with He Init and LeakyReLU built successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Batch Normalization\n",
    "Adding Batch Normalization layers is usually done **after** the linear computation and **before** the activation function (though after activation also works). This stabilizes training and allows higher learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bn = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    \n",
    "    # Input normalization is implicit in the first BN layer if placed here\n",
    "    keras.layers.BatchNormalization(),\n",
    "    \n",
    "    keras.layers.Dense(300, use_bias=False), # Bias is redundant because BN adds a shift parameter (beta)\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    \n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    \n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# BN models often benefit from larger learning rates\n",
    "model_bn.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                 optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "                 metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Model with Batch Normalization built successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Transfer Learning\n",
    "Instead of training from scratch, we often reuse the lower layers of a pretrained model (e.g., trained on ImageNet) and retrain the upper layers for our specific task. \n",
    "\n",
    "Here, we simulate this by taking a trained `model_A` and creating `model_B` on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating a pretrained model A\n",
    "model_A = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "model_A.fit(X_train[:1000], y_train[:1000], epochs=1, verbose=0) # Quick pretraining\n",
    "\n",
    "# Reuse layers for Model B (Transfer Learning)\n",
    "# We reuse all layers except the output layer\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\")) # New binary classification head\n",
    "\n",
    "# Freeze reused layers to prevent destroying their weights during the first few epochs\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "print(\"Transfer Learning model ready. Reused layers are frozen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Optimizers and Learning Rate Scheduling\n",
    "We will demonstrate the **Nadam** optimizer (Adam + Nesterov Momentum) combined with an **Exponential Decay** schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Schedule: Exponential Decay\n",
    "# decayed_learning_rate = initial_learning_rate * decay_rate ^ (step / decay_steps)\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "# Nadam Optimizer: Often converges faster than standard SGD or Adam\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=lr_schedule)\n",
    "\n",
    "model_opt = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_opt.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Model configured with Nadam optimizer and Exponential Decay LR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Regularization: Dropout\n",
    "Implementing **Dropout**, the most popular regularization technique for DNNs. A dropout rate of 0.2 means 20% of neurons are ignored at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    \n",
    "    # Dropout layer applied after activation\n",
    "    # Ideally placed after every dense layer\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    \n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    \n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_dropout.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                      optimizer=\"adam\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the dropout model\n",
    "# Dropout is only active during training, not during evaluation/testing.\n",
    "history = model_dropout.fit(X_train, y_train, epochs=5, \n",
    "                            validation_data=(X_valid, y_valid))\n",
    "\n",
    "print(\"Training with Dropout complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}